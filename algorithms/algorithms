Introduction to Algorithms

  Each chapter presents an algorithm, a design technique, an application area,
  or a related topic.

* The Role of Algorithms in Computing

  An algorithm is any well-defined computational procedure that takes some
  value, or set of values, as input and produces some value, or set of values,
  as output. An algorithm is thus a sequence of computational steps that
  transform the input into the output.

  In general, an instance of a problem consists of the input (satisfying
  whatever constraints are imposed in the problem statement) needed to compute a
  solution to the problem.

  An algorithm is said to be correct if, for every input instance, it halts with
  the correct output. We say that a correct algorithm solves the given
  computational problem. An incorrect algorithm might not halt at all on some
  input instances, or it might halt with an incorrect answer. Contrary to what
  you might expect, incorrect algorithms can sometimes be useful, if we can
  control their error rate.


  Not every problem solved by algorithms has an easily identified set of
  candidate solutions.


- Data structures

  A data structure is a way to store and organize data in order to facilitate
  access and modifications. No single data structure works well for all
  purposes, and so it is important to know the strengths and limitations of
  several of them.

  Most of this book is about efficient algorithms. Our usual measure of
  efficiency is speed, i.e., how long an algorithm takes to produce its result.
  There are some problems, however, for which no efficient solution is known.
  Chapter 34 studies an interesting subset of these problems, which are known as
  NP-complete.

  Why are NP-complete problems interesting? First, although no efficient algo-
  rithm for an NP-complete problem has ever been found, nobody has ever proven
  that an efficient algorithm for one cannot exist. In other words, no one knows
  whether or not efficient algorithms exist for NP-complete problems. Second,
  the set of NP-complete problems has the remarkable property that if an
  efficient algo- rithm exists for any one of them, then efficient algorithms
  exist for all of them.


  You should know about NP-complete problems because some of them arise sur-
  prisingly often in real applications. If you are called upon to produce an
  efficient algorithm for an NP-complete problem, you are likely to spend a lot
  of time in a fruitless search. If you can show that the problem is
  NP-complete, you can instead spend your time developing an efficient algorithm
  that gives a good, but not the best possible, solution.

  As a concrete example, consider a delivery company with a central depot. Each
  day, it loads up each delivery truck at the depot and sends it around to
  deliver goods to several addresses. At the end of the day, each truck must end
  up back at the depot so that it is ready to be loaded for the next day. To
  reduce costs, the company wants to select an order of delivery stops that
  yields the lowest overall distance traveled by each truck. This problem is the
  well-known “traveling-salesman problem,” and it is NP-complete. It has no
  known efficient algorithm. Under certain assumptions, however, we know of
  efficient algorithms that give an overall distance which is not too far above
  the smallest possible.


  Different algorithms devised to solve the same problem often differ
  dramatically in their efficiency. These differences can be much more
  significant than differences due to hardware and software.


* Getting Started

  - Insertion Sort

    We start with insertion sort, which is an efficient algorithm for sorting a
    small number of elements.

    We present our pseudocode for insertion sort as a procedure called INSERTION
    - SORT, which takes as a parameter an array A[1..n] containing a sequence of
    length n that is to be sorted. (In the code, the number n of elements in A
    is denoted by A:length.) The algorithm sorts the input numbers in *place*:
    it rearranges the numbers within the array A, with at most a constant number
    of them stored outside the array at any time. The input array A contains the
    sorted output sequence when the INSERTION -SORT procedure is finished.

        for j = 2 to A.length {
            key = A[j]
            // Insert A[j] into the sorted sequence A[1..j-1]
            i = j - 1

            while i > 0 and A[i] > key {
                    A[i + 1] = A[i]
                    i = i - 1;
            }

            A[i + 1] = key
        }


        e.g

        5       2       4       5       1       3

        2       5       4       6       1       3

        2       4       5       6       1       3

        2       4       5       6       1       3

        1       2       4       5       6       3

        1       2       3       4       5       6

  We state these properties of A[1..j - 1] formally as a loop invariant:

  At the start of each iteration of the for loop of lines 1–8, the subarray
  A[1..j - 1] consists of the elements originally in A[1.. j -1 ] but in sorted
  order.


  We use loop invariants to help us understand why an algorithm is correct. We
  must show three things about a loop invariant:

    Initialization: It is true prior to the first iteration of the loop

    Maintenance:    If it is true before an iteration of the loop, it remains
                    true before the next iteration.

    Termination:    When the loop terminates, the invariant gives us a useful
                    property that helps show that the algorithm is correct.


  When the first two properties hold, the loop invariant is true prior to every
  iteration of the loop.


  Note the similarity to mathematical induction, where to prove that a property
  holds, you prove a base case and an inductive step. Here, showing that the
  invariant holds before the first iteration corresponds to the base case, and
  showing that the invariant holds from iteration to iteration corresponds to
  the inductive step.

  The third property is perhaps the most important one, since we are using the
  loop invariant to show correctness. Typically, we use the loop invariant along
  with the condition that caused the loop to terminate. The termination property
  differs from how we usually use mathematical induction, in which we apply the
  inductive step infinitely; here, we stop the “induction” when the loop
  terminates.

  Let us see how these properties hold for insertion sort.


  Initialization: when j = 2. The subarray A[1.. j -1] consists of just the
                  single element A[1], which is in fact the original element in
                  A[1].  Array already sorted prior to the first iteration of
                  the loop.

  Maintenance:    We must show that each iteration maintains the loop invariant.

                  Informally, the body of the for loop works by moving A[j-1],
                  A[j-2], A[j-3], and so on by one position to the right until
                  it finds the proper position for A[j].

                  The subarray A[1..j]  then consists of the elements originally
                  in A[1..j], but in sorted order.

                  Incrementing j for the next iteration of the for loop then
                  preserves the loop invariant.

  Termination:    Finally, we examine what happens when the loop terminates.

                  The condition causing the for loop to terminate is that j >
                  A.length = n. Because each loop iteration increases j by 1, we
                  must have j = n + 1 at that time.

                  Substituting n + 1 for j in the wording of loop invariant, we
                  have that the subarray A[1..n]  consists of the elements
                  originally in A[1..n], but in sorted order.


- Analyzing Algorithms

  Analyzing an algorithm has come to mean predicting the resources that the
  algorithm requires. Occasionally, resources such as memory, communication
  bandwidth, or computer hardware are of primary concern, but most often it is
  computational time that we want to measure. Generally, by analyzing several
  candidate algorithms for a problem, we can identify a most efficient one. Such
  analysis may indicate more than one viable candidate, but we can often discard
  several inferior algorithms in the process.


  For most of this book, we shall assume a generic one-processor, random-access
  machine (RAM) model of computation as our implementation technology and
  understand that our algorithms will be implemented as computer programs. In
  the RAM model, instructions are executed one after another, with no
  concurrent operations.


  In the RAM model, we do not attempt to model the memory hierarchy that is
  common in contemporary computers. That is, we do not model caches or virtual
  memory. Several computational models attempt to account for memory-hierarchy
  effects, which are sometimes significant in real programs on real machines. A
  handful of problems in this book examine memory-hierarchy effects, but for the
  most part, the analyses in this book will not consider them. Models that
  include the memory hierarchy are quite a bit more complex than the RAM model,
  and so they can be difficult to work with. Moreover, RAM-model analyses are
  usually excellent predictors of performance on actual machines.


- Analysis of insertion sort

  The time taken by the INSERTION-SORT procedure depends on the input: sorting a
  thousand numbers takes longer than sorting three numbers. Moreover,
  INSERTION-SORT can take different amounts of time to sort two input sequences
  of the same size depending on how nearly sorted they already are. In general,
  the time taken by an algorithm grows with the size of the input, so it is
  traditional to describe the running time of a program as a function of the
  size of its input. To do so, we need to define the terms “running time” and
  “size of input” more carefully.


  The best notion for input size depends on the problem being studied. For many
  problems, such as sorting or computing discrete Fourier transforms, the most
  natural measure is the number of items in the input—for example, the array
  size n for sorting. For many other problems, such as multiplying two integers,
  the best measure of input size is the total number of bits needed to represent
  the input in ordinary binary notation. Sometimes, it is more appropriate to
  describe the size of the input with two numbers rather than one. For instance,
  if the input to an algorithm is a graph, the input size can be described by
  the numbers of vertices and edges in the graph. We shall indicate which
  input size measure is being used with each problem we study.

  The running time of an algorithm on a particular input is the number of
  primitive operations or “steps” executed.

  For the moment, let us adopt the following view. A constant amount of time is
  required to execute each line of our pseudocode.

   *** When a for or while loop exits in the usual way, (due to the test in the
   loop header), the test is executed one time more than the loop body.



       Insertion-Sort                       cost    times

       for j = 2 to A.length                c1       n
               key = A[j]                   c2       n - 1
               //insert A[j] into
               // sorted sequence
               // A[1..j-1]                 0        n - 1
               i = j -1                     c4       n - 1
               while i>0 and A[i] > key     c5       summation ^n  _j=2 t_j
                       A[i+1] = A[i]        c6       summation ^n _j=2  t_j -1
                       i = i - 1            c7       summation ^n _j=2  t_j -1
               A[i+1] = key                 c8       n - 1


  The running time of the algorithm is the sum of running times for each state-
  ment executed; a statement that takes ci steps to execute and executes n times
  will contribute ci n to the total running time.6 To compute T .n/, the running
  time of I NSERTION -S ORT on an input of n values, we sum the products of the
  cost and times columns, obtaining


    T(n) = c1 n + c2 (n - 1) + c4 (n - 1) + c5 sum^n _j=2 tj 
            + c6 sum^n _j=2 (tj-1) + c7 sum^n _j=2 (tj-1) + c8 ( n - 1)


  Even for inputs of a given size, an algorithm’s running time may depend on
  which input of that size is given. For example, in I NSERTION -S ORT, the best
  case occurs if the array is already sorted. For each j = 2, 3,...n, we then
  find that A[i] <= key in  line 5 when i has its initial value of j - 1. Thus
  tj = 1 for j = 2, 3,...n, and the best-case running time is

     T(n) = c1 n + c2 (n - 1) + c4 (n - 1) + c5 ( n - 1) + c8 ( n - 1)

           [ c6 anc c7 are not executed]

          = (c1 + c2 + c4 + c5 + c8)n - (c2 + c4 + c5 + c8)


  We can express this running time as  an + b for constants a and b that depend
  on the statement costs ci; it is thus a linear function of n.

  If the array is in reverse sorted order—that is, in decreasing order—the worst
  case results. We must compare each element A[j] with each element in the
  entire sorted subarray A[i..j-1], and so tj = j for j = 2,3,...n. Noting that


         Sum ^n _j=2 j =     [n(n+1)]/2 -1

         and

         Sum ^n _j=2 (j -1) = [n(n -1) ] /2


         T(n)  = [after simplifications]

               = a n^2 + b n + c


  We can express this worst-case running time as a n^2 + b n + c for constants
  a, b and c that again depends on the statement costs ci; it is thus a
  quadratic function of n.

  Typically, as in insertion sort, the running time of an algorithm is fixed for
  a given input, although in later chapters we shall see some interesting
  “randomized” algorithms whose behavior can vary even for a fixed input.


- Worst-case and average-case analysis

  In our analysis of insertion sort, we looked at both the best case, in which
  the input array was already sorted, and the worst case, in which the input
  array was reverse sorted. For the remainder of this book, though, we shall
  usually concentrate on finding only the worst-case running time, that is, the
  longest running time for any input of size n. We give three reasons for this
  orientation.

  The worst-case running time of an algorithm gives us an upper bound on the
  running time for any input. Knowing it provides a guarantee that the algorithm
  will never take any longer. We need not make some educated guess about the
  running time and hope that it never gets much worse.

  For some algorithms, the worst case occurs fairly often. For example, in
  search- ing a database for a particular piece of information, the searching
  algorithm’s worst case will often occur when the information is not present in
  the database.  In some applications, searches for absent information may be
  frequent.

  The “average case” is often roughly as bad as the worst case. Suppose that we
  randomly choose n numbers and apply insertion sort. How long does it take to
  determine where in subarray A[1..j-1] to insert element A[j]? On average, half
  the elements in A[1..j-1] are less than A[j], and half the elements are
  greater. On average, therefore, we check half of the subarray A[1..j -1], and
  so tj is about j/2. The resulting average-case running time turns out to be a
  quadratic function of the input size, just like the worst-case running time.

  In some particular cases, we shall be interested in the average-case running
  time of an algorithm; we shall see the technique of probabilistic analysis
  applied to various algorithms throughout this book. The scope of average-case
  analysis is limited, because it may not be apparent what constitutes an
  “average” input for a particular problem. Often, we shall assume that all
  inputs of a given size are equally likely. In practice, this assumption may be
  violated, but we can sometimes use a randomized algorithm, which makes random
  choices, to allow a probabilistic analysis and yield an expected running time.
  We explore randomized algorithms more in Chapter 5 and in several other
  subsequent chapters.


- Order of growth

  We used some simplifying abstractions to ease our analysis of the INSERTION -
  SORT procedure. First, we ignored the actual cost of each statement, using
  the constants ci to represent these costs. Then, we observed that even these
  constants give us more detail than we really need: we expressed the worst-case
  running time as an^2 + bn + c for some constants a, b, and c that depend on
  the statement costs ci . We thus ignored not only the actual statement costs,
  but also the abstract costs ci.

  We shall now make one more simplifying abstraction: it is the rate of growth,
  or order of growth, of the running time that really interests us. We therefore
  con- sider only the leading term of a formula (e.g a n^2), since the
  lower-order terms are relatively insignificant for large values of n. We also
  ignore the leading term’s con- stant coefficient, since constant factors are
  less significant than the rate of growth in determining computational
  efficiency for large inputs. For insertion sort, when we ignore the
  lower-order terms and the leading term’s constant coefficient, we are left
  with the factor of  n^2 from the leading term.

  We write that insertion sort has a worst-case running time of 0(n^2) . We
  shall use the O-notation informally in this chapter.


  We usually consider one algorithm to be more efficient than another if its
  worst- case running time has a lower order of growth. Due to constant factors
  and lower- order terms, an algorithm whose running time has a higher order of
  growth might take less time for small inputs than an algorithm whose running
  time has a lower order of growth. But for large enough inputs, a 0(n^2)
  algorithm, for example, will run more quickly in the worst case than a 0(n^3)
  algorithm.


- Designing algorithms

  We can choose from a wide range of algorithm design techniques. For insertion
  sort, we used an incremental approach: having sorted the subarray A[1..j-1],
  we inserted the single element A[j] into its proper place, yielding the sorted
  subarray A[1..j].

  In this section, we examine an alternative design approach, known as “divide-
  and-conquer,” which we shall explore in more detail in Chapter 4. We’ll use
  divide- and-conquer to design a sorting algorithm whose worst-case running
  time is much less than that of insertion sort. One advantage of
  divide-and-conquer algorithms is that their running times are often easily
  determined using techniques that we will see in Chapter 4.


- Divide-and-conquer approach

  Many useful algorithms are recursive in structure: to solve a given problem,
  they call themselves recursively one or more times to deal with closely
  related sub- problems. These algorithms typically follow a divide-and-conquer
  approach: they break the problem into several subproblems that are similar to
  the original prob- lem but smaller in size, solve the subproblems recursively,
  and then combine these solutions to create a solution to the original problem.

  The divide-and-conquer paradigm involves three steps at each level of the
  recursion:


    Divide      the problem into a number of subproblems that are smaller
                instances of the same problem.

    Conquer     the subproblems by solving them recursively. If the subproblem
                sizes are small enough, however, just solve the subproblems in a
                straightforward manner.

    Combine     the solutions to the subproblems into the solution for the
                original problem.


  The merge sort algorithm closely follows the divide-and-conquer paradigm. In-
  tuitively, it operates as follows.

    Divide      Divide the n-element sequence to be sorted into two subsequences
                of n/2 elements each.

    Conquer     Sort the two subsequences recursively using merge sort

    Combine     Merge the two sorted subsequences to produce the sorted answer.


  The recursion “bottoms out” when the sequence to be sorted has length 1, in
  which case there is no work to be done, since every sequence of length 1 is
  already in sorted order.


  The key operation of the merge sort algorithm is the merging of two sorted
  sequences in the “combine” step. We merge by calling an auxiliary procedure
  Merge(A, p, q, r), where A is an array and p, q, and r are indices into the
  array such that p <= q< r. The procedure assumes that the subarrays A[p..q]
  and A[q +1..r] are in sorted order. It merges them to form a single sorted
  subarray that replaces the current subarray A[p..r].

  Our Merge procedure takes time 0(n), where n = r - p + 1 is the total number
  of elements being merged, and it works as follows.

  Suppose we have two piles of cards face up on a table. Each pile is sorted,
  with the smallest cards on top. We wish to merge the two piles into a single
  sorted output pile, which is to be face down on the table. Our basic step
  consists of choosing the smaller of the two cards on top of the face-up piles,
  removing it from its pile (which exposes a new top card), and placing this
  card face down onto the output pile. We repeat this step until one input pile
  is empty, at which time we just take the remaining input pile and place it
  face down onto the output pile.

  Computationally, each basic step takes constant time, since we are comparing
  just the two top cards. Since we perform at most n basic steps, merging takes
  0(n) time.

  The following pseudocode implements the above idea, but with an additional
  twist that avoids having to check whether either pile is empty in each basic
  step.  We place on the bottom of each pile a sentinel card, which contains a
  special value that we use to simplify our code. Here, we use INF as the
  sentinel value, so that whenever a card with INF is exposed, it cannot be the
  smaller card unless both piles have their sentinel cards exposed. But once
  that happens, all the nonsentinel cards have already been placed onto the
  output pile. Since we know in advance that exactly r - p + 1 cards will be
  placed onto the output pile, we can stop once we have performed that many
  basic steps.

     [ so basically from p to q, the array is sorted, and from q+1 to r, the
     array is sorted]

        p          q               r

        1 2 3 4 5  4 5 6 10 11 12 14

       MERGE(A, p, q, r)

           n1 = q - p + 1
           n2 = r - q

           let L[1..n1 + 1] and R[1..n2 + 1] be new arrays

           for i = 1 to n1
               L[i] = A[p + i - 1]

           for j = 1 to n1
               R[j] = A[q + j]

           L[n1 + 1] = INF
           R[n2 + 1] = INF

           i = 1
           j = 1

12         for k = p to r
               if L[i] <= R[j]
                   A[k] = L[i]
                   i = i + 1
               else A[k] = R[j]
17             j = j + 1


  We must show that this loop invariant holds prior to the first iteration of
  the for loop of lines 12–17, that each iteration of the loop maintains the
  invariant, and that the invariant provides a useful property to show
  correctness when the loop terminates.

  Initialization: Prior to the first iteration of the loop, we have k = p, so
                  that the subarray A[p..k -1] is empty. This empty subarray
                  contains the k - p = 0 [so [k -1] - p + 1 ] smallest elements
                  of L and R, and since i = j = 1, both L[i] and R[j] are the
                  smallest elements of their arrays that have not been copied
                  back into A.

   Maintenance:   To see that each iteration maintains the loop invariant, let
                  us first suppose that L[i] <= R[j]. Then L[i] is the smallest
                  element not yet copied back into A. Because A[p..k-1] contains
                  the k-p smallest elements, after line 14 copies L[i] into
                  A[k], the subarray A[p..k] will contain the k - p + 1 smallest
                  elements. Incrementing k and i reestablishes the loop
                  invariant for the next iteration.

                  If instead L[i] > R[j], then lines 16-17 perform the
                  appropriate action to maintain the loop invariant.

   Termination:   At termination k = r + 1. By the loop invariant, the subarray
                  A[p.. k-1], which is A[p..r], contains the k - p = r - p + 1
                  smallest elements of L[1..n1 + 1] and R[1..n2+1] in sorted
                  order. The arrays L and R together conain n1 + n2 + 2 = r - p
                  + 3 elements. All but the two largest have been copied back
                  into A, and these two largest elements are the sentinels.



  To see that the MERGE procedure runs in 0(n time, where n = r - p + 1, observe
  that each of lines 1-3 and 8-11 takes constant time, the for loops of lines
  4-7 takes 0(n1 + n2) = 0(n) time, and there are n iterations of the for loops
  of lines 12-17, each of which takes constant time.

  We can now use the M ERGE procedure as a subroutine in the merge sort al-
  gorithm. The procedure MERGE-SORT(A, p, r) sorts the elements in the subarray
  A[p..r].

  If p >= r, the subarray has at most one element and is therefore already
  sorted. Otherwise, the divide step simply computes and index q that partitions
  A[p..r] into two subarrays: A[p..q], containing [n/2] elements, and A[q
  +1..r], containing [n/2] elements.


     MERGE-SORT (A, p, r)

         if p < r

             q = (p+r)/2
             MERGE-SORT(A,p,q)
             MERGE-SORT(A,q+1,r)
             MERGE(A,p,q,r)


  To sort the entire sequence, we make the initial call MERGE-SORT(A,1,
  A.length), where once again A.length = n.

  The algorithm consists of merging pairs of 1-item sequences to form sorted
  sequences of length 2, merging pairs of sequences of length 2 to form sorted
  sequences of length 4, and so on, until two sequences of length n/2 are merged
  to form the final sorted sequence of length n.

                         sorted sequence

                         1 2 2 3 4 5 6 7

                             merge

                  2 4 5 7                  1 2 3 6

                   merge                    merge

          2 5            4 7          1 3          2 6

     5         2      4        7   1        3    2     6

                      initial sequence


  A recurrence for the running time of a divide-and-conquer algorithm falls out
  from the three steps of the basic paradigm. As before, we let T (n) be the
  running time on a problem of size n.

  If the problem size is small enough, say n <= c for some constant c, the
  straightforward solution takes constant time, which we write as 0(1).


  Suppose that our division of the problem yields a subproblems, each of which
  is 1/b the size of the original.  (For merge sort, both a and b are 2, but we
  shall see many divide-and-conquer algorithms in which a != b.)

  It takes time T (n/b) to solve one subproblem of size n/b, and so it takes
  time a T (n/b) to solve a of them. If we take D(n) time to divide the problem
  into subproblems and C(n) time to combine the solutions to the subproblems
  into the solution to the original problem, we get the recurrence

                  0(1)                        if n <= c
     T(n)  =      a T (n/b) + D(n) + C(n)     otherwise - Analysis of merge sort

  Although the pseudocode for MERGE-SORT works correctly when the number of
  elements is not even, our recurrence-based analysis is simplified if we assume
  that the original problem size is a power of 2. Each divide step then yields
  two subse- quences of size exactly n/2.

  We reason as follows to set up the recurrence for  T(n), the worst-case
  running time of merge sort of n numbers. Merge sort on just one element takes
  constant time.  When we have n > 1 elements, we break down the running time as
  follows:


  Divide: the divide step just computes the middle of the subarray, which takes
          constant time. Thus D(n) = 0(1).

  Conquer: We recursively solve two subproblemes, each of size n/2, which
           contributes 2 T(n/2) to the running time.

  Combine: We have already noted that the MERGE procedure on an n-element
           subarray takes time 0(n) and so C(n) = 0(n).

  When we add the functions  D(n) and C(n) for the merge sort analysis, we are
  adding a function that is 0(n) and a function that is 0(1). This sum is a
  linear function of n, that is, 0(n).

  Adding it to the 2T(n/2) term from the "conquer" step gives the recurrence for
  the worst-case running time T(n) of merge sort:


   T(n)   = 0(1)            if n = 1
            2 T(n/2) + 0(n) if n > 1

  Because the logarithm function grows more slowly than any linear function,
  for large enough inputs, merge sort, with its 0(n lg n) running time,
  outperforms insertion sort, whose running time is 0(n^2), in the worst case.


                                cn                              cn

    .                   cn/2          cn/2                      cn
    lg n
    .    cn/4            cn/4        cn/4          cn/4         cn
    .
    .     .               .           .              .
    .     .               .           .              .
    .     .               .           .              .
    .     c               c           c              c          cn




                                cn

                     T(n/2)           T(n/2)






                                cn

                     cn/2             cn/2


      T(n/4)          T(n/4)          T(n/4)          T(n/4)


  Each level i below the top has 2^i nodes, each conrtributing a cosr of c(n /
  2^i), so that the i th level below the top has total cost 2 ^ i c (n/ 2^i) =
  cn. The bottom level has n nodes, each contributng a cost of c, for a total
  cost of cn.

  The total cost of levels of recursion tree is lg n + 1, where n is the number
  of leaves, corresponding to the input size.

  So cn (lg n  + 1) = cn lg n + cn.

  Ignoring the low-order term and the constant c gives the desired result of 
  0(n lg n)

        let say n  = 16, it will become 8 , 4 , 2, and eventually 1.

        4 steps === log_2 16
        at each step, it takes n for merging

        So total = cn lg n + cn
    [ think with a small array, say of size 4, that algorithm will make more
    sense]
    pg 43 but not understood merge sort


* Growth of Functions

  The order of growth of the running time of an algorithm, gives a simple
  characterization of the algorithm's efficiency and also allows us to compare
  the relative performance of alternative algorithms. Once the input size n
  becomes large enoughm merge sort, with its 0(n lg n) worst-case running time,
  beats insertion sort, whose worst-case running time is 0(n^2).

  Although we can sometimes determine the exact running time of an algorithm, as
  we did for insertion sort, the extra precision is not usually worth the effort
  of computing it.

  For large enough inputs, the multiplicative constants and lower-order terms of
  an exact running time are dominated by the effect of the input size itself.

  When we look at input sizes large enough to make only the order of growth of
  the running time relevant, we are studying the asymptotic efficiency of
  algorithms.  That is, we are concerned with how the running time of an
  algorithm increases with the size of the input in the limit, as the size of
  the input increases without bound.  Usually, an algorithm that is
  asymptotically more efficient will be the best choice for all but very small
  inputs.

  This chapter gives several standard methods for simplifying the asymptotic
  analysis of algorithms. The next section begins by defining several types of
  “asymptotic notation,” of which we have already seen an example in O-notation.

- Asymptotic notation

  Such notation are convenient for describing the worst-case running-time
  function T(n), which usually is defined only on integer input sizes.

  We sometimes find it convenient, however, to abuse asymptotic notation in a
  variety of ways.

  We might extend the notation to the domain of real numbers or, alternatively,
  restrict it to a subset of the natural numbers.


- Asymptotic notation, functions, and running times

  We will use asymptotic notation primarily to describe the running times of
  algorithms, as when we wrote that insertion sort’s worst-case running time
  is 0(n^2).

  Asymptotic notation actually applies to functions, however. Recall that we
  characterized insertion sort’s worst-case running time as a n ^2 + b n + c,
  for some constants a,b, and c. By writing that insertion sort's running time
  as  0(n^2), we abstracted away some details of this function.

  In this book, the functions to which we apply asymptotic notation will usually
  characterize the running times of algorithms. But asymptotic notation can
  apply to functions that characterize some other aspect of algorithms (the
  amount of space they use, for example), or even to functions that have nothing
  whatsoever to do with algorithms.

  Even when we use asymptotic notation to apply to the running time of an al-
  gorithm, we need to understand which running time we mean. Sometimes we are
  interested in the worst-case running time. Often, however, we wish to
  characterize the running time no matter what the input. In other words, we
  often wish to make a blanket statement that covers all inputs, not just the
  worst case. We shall see asymptotic notations that are well suited to
  characterizing running times no matter what the input.


- 0-notation  [ In the book replace O by an 0 with a dot inside]

  In chapter 2, we found that the worst-case running time of insertion sort s
  T(n) = 0(n^2). Let us define what this notation means.

  For a given function g(n), we denote by 0( g(n) ) the set of functions


  0( g(n) ) = { f(n) : there exists positive constants c1, c2, and n0 such that
                        0 <= c1 g(n) <= f(n) <= c2 g(n) for all n >= n0

                        [ pg 45 for graph] [ shows f(n) in between c2 g(n) and
                        c1 g(n) after point n0]

                        f(n) is "sandwiched" between c1 g(n) and c2 g(n), for
                        sufficiently large n.

 Because 0(g(n) is a set, we could write f(n) E 0(g(n)) to indicate that f(n) is
 a member of 0(g(n)).

 Instead, we will usually write f(n) = 0(g (n)) to express the same notion.


 We say that g(n) is an *asymptotically tight bound* for f(n).

 The definition of 0(g(n)) requires that every member f(n) E 0(g(n)) be
 asymptotically nonnegative, that is, that f(n) be nonnegative whenever n is
 sufficiently large.  [Why? Look at how they defined the function 0 <=... ]

 (An asymptotically positive function is one that is positive for all
 sufficiently large n).

  Consequently, the function g(n) itself must be asymptotically nonnegative, or
  else the set 0(g(n)) is empty.

  We shall therefore assume that every function used within the 0-notation is
  asymptotically nonnegative.


  Reason why we threw the lower terms away and ignore the leading coefficient of
  the highest-order term.

        Let say 0(n^2) = 0.5 n^2 - 3n

        c1 n^2 <= 0.5 n^2 - 3n <= c2 n^2

        for all n >= n0. Dividing by n^2 yields

                c1 <= 0.5 - 3/n <= c2


  We can make the right-hand inequality hold for any value of n >= 1 by choosing
  any constant c2 >= 0.5. Likewise, we can make the left-hand inequality hold
  for any value of n >= 7 by choosing any constant c1 <= 1/14

  Thus, by choosing c1 = 1/14, c2 = 0.5 and n0 = 7, we can verify that 
  0.5n^2 - 3n = 0(n^2)

    So f(n) = 0.5n^2 - 3n
        g(n) = n^ 2

  Certainly, other choices for the constants exist, but the important thing is
  that some choice exists.

  We can also use the formal definition to verify that 6n^3 != 0(n^2). Suppose

        6n^3 <= c2 n^2 for all n >= n0.

        divide by n^2

        n <= c2 / 6, which cannot possibly hold for arbitrarily large n, since
        c2 is constant.

  Intuitively, the lower-order terms of an asymptotically positive function can
  be ignored in determining asymptotically tight bounds because they are
  insignificant for large n. When n is large, even a tiny fraction of the
  highest-order term suf- fices to dominate the lower-order terms.

  Setting c1 slightly lower that the coefficient of the highest-order term and
  setting c2 to a value slightly larger permits the inequalities in the definion
  of 0-notation to be satisfied.

  The coefficient of the highest-order term can likewise be ignored, since it
  only changes c1 and c2 by a constant factor equal to the coefficient.

     In general, for any polynomial p(n) = Sum ^d _ i =0 a_i n^i, we have

                    p(n) = 0(n^d)

      Since any constant is a degree-0 polynomial, we can express any constant
      function as 0(n^0) or 0(1).

      The latter notation is a minor abuse, however, because the expression does
      not indicate what variable is tending to infinity.


- O- notation

  The 0‚-notation asymptotically bounds a function from above and below. When we
  have only an asymptotic upper bound, we use O-notation. For a given func- tion
  g.n/, we denote by O(g(n)) (pronounced “big-oh of g of n” or sometimes just
  “oh of g of n”) the set of functions



      O(g(n)) = { f(n): there exists positive constants c and n_0 such that
                        0<= f(n) <= c g(n) for all n >= n_0}


  We use O-notation to give an upper bound on a function, to within a constant
  factor.


  For all values n at and to the right of n0 , the value of the function f(n) is
  on or below cg(n).

         [graph with cg(n) higher than f(n)]

  We write f(n) = O(g(n)) to indicate that a function f(n) is a member of the
  set O(g(n)).


  *** Note that f(n) = 0(g(n)) implies f(n) = O(g(n)), since ‚0-notation is a
  stronger notion than O-notation.

  Written set-theoretically, we have

        0(g(n))  C= O(g(n)).

         [ C= is the set notation thing]pg 47

  Thus, our proof that any qyadratic function an^2 + bn + c, where a > 0, is in
  0(n^2) also shows that any quadratic function is in O(n^2).

  What may be surprising is that when a > 0, any linear function an + b is in
  O(n^2), which is easily verified by taking c = a + |b| and n0 = max(1, -b/a).


  If you have seen O-notation before, you might find it strange that we should
  write, for example, n = O(n^2). In this literature, we sometimes find
  O-notation informally describing asymptotically tight bounds, that is, what we
  have defined using 0-notation.

  In this book, however, we write f(n) = O(g(n)), we are merely claiming that
  some constant multiple of g(n) is an asymptotic upper bound of f(n), with no
  claim about how tight an upper bound is.


   Distinguishing asymptotic upper bounds from asymptotically tight bounds is
   standard in the algorithms literature.

   Using O-notation, we can often describe the running time of an algorithm
   merely by inspecting the algorithm's overall structure.

   *** For example, the doubly nested loop structure in the insertion sort
   algorithm immediately yields an O(n^2) upper bound on the worst-case running
   time: the cost of each iteration of the inner loop is bounded from above by
   O(1) [ constant ], the indices i and j are both at most n, and the inner loop
   is executed at most once for each of the n^2 pairs of values for i and j.


   Since O-notation describes an upper bound, when we use it to bound the
   worst-case running time of an algorithm, we have a bound on the running time
   of the algorithm on every input - the blanket statement we discussed earlier.
   Thus, the O(n^2) bound on worst-case running time of insertion sort also
   applies to its running time on every input.

   The 0(n^2) bound on the worst-case running time of insertion sort, however,
   does not imply a 0(n^2) bound on the running time of insertion sort on
   *every* input.

   [So 0(n^2) does not apply to *every* input but is probably the worst case
   scenario.]

   For example, we saw that when the input is already sorted, insertion sort
   runs in 0(n) time.


   Technically, it is an abuse to say that the running time of insertion sort is
   O(n^2), since for a given n, the actual running time varies, depending on the
   particular input of size n.

   When we say "the running time is O(n^2)," we mean that there is a function
   f(n) that is O(n^2) such that for any value of n, no matter what particular
   input of size n is chosen, the running time on that input is bounded from
   above by the value of f(n). Equivalently, we mean that the worst-case running
   time is O(n^2).

- Ω-notation

  Ω-notation provides an asymptotic lower bound. For a given function g(n), we
  denote by Ω(g(n)) (pronounced "big-omega of g of n" or sometimes just "omega
  of g of n") the set of functions


    Ω(g(n)) = { f(n): there exist positive constants c and n0 such that
                      0 <= cg(n) <= f(n) for all n >= n0}

       [ graph that shows g(n) below f(n) ]
       [ for n > n0, the value of f(n) is on or above cg(n) ]


  Theorem 3.1

  For any two functions f(n) and g(n), we have f(n) = 0(g(n)) if and only if

        f(n) = O(g(n)) and f(n) = Ω(g(n))


  As an example of the application of this theorem, our proof that an^2 + bn + c
  = 0(n^2) for any constants a, b, and c, where a > 0,

  immediately implies that an^2 + bn + c = Ω(n^2) and an^2 + bn + c = O(n^2).

  In practice, rather than using Theorem 3.1 to obtain asymptotic upper and
  lower bounds from asymptotically tight bounds, as we did for this example, we
  usually use it to prove asymptotically tight bounds from asymptotic upper and
  lower bounds.


  When we say that the running time (no modifer) of an algorithm is Ω(g(n)), we
  mean that no matter what particular input of size n is chosen for each value
  of n, the running time on that input is at least a constant times g(n)m for
  sufficiently large n.

  #############################################################################
  Equivalently, we are giving a lower bound on the best-case running time of an
  algorithm. For example, the best-case running time of insertion sort is Ω(n),
  which implies that the running time of insertion sort is Ω(n).
  #############################################################################


  ##############################################################################
  The running time of insertion sort therefore belongs to both Ω(n) and O(n^2),
  since it falls anywhere between a linear function of n and a quadratic
  function of n. Moreover, these bounds are asymptotically as tight as possible:
  for instance, the running time of insertion sort is not Ω(n^2), since there
  exists an input for which insertion sort runs in 0(n) time (e.g, when the
  input is already sorted).
  ##############################################################################

  It is not contradictory, however, to say the worst-case running time of
  insertion sort is Ω(n^2), since there exists an input that causes the
  algorithm to take Ω(n^2) time.


- Asymptotic notation in equations and inequalities

  We have already seen how asymptotic notation can be used within mathematical
  formulas. For example, in introducing O-notation, we wrote "n = O(n^2)" We
  might also write 2n^2 + 3n + 1 = 2 n^2 + 0(n).

  How do we interpret such formulas?

  When the asymptotic notation stands alone (i.e, not within a larger formula)
  on the right-hand side of an equation (or inequality), as in n = O(n^2), we
  have already defined the equal sign to mean set membership: n E O(n^2). In
  general, however, when asymptotic notation appears in a formula, we interpret
  it as standing for some anonymous function that we do not care to name.

  For example, the formula 2n^2 + 3n + 1 = 2n^2 + 0(n) means that 2n^2 + 3n + 1
  = 2n^2 + f(n), where f(n) is some function in the set 0(n). In this case, we
  let f(n) = 3n + 1, which indeed is in 0(n).

  *** Using asymptotic notation in this manner can help eliminate inessential
  detail and clutter in an equation. For example, we expressed the worst-case
  running time of merge sort as the recurrence

           T(n) = 2 T(n/2) + 0(n).

  If we are interested only in the asymptotic behavior of T(n), there is no
  point in specifying all the lower-order terms exactly; they are all understood
  to be included in the anonymous function denoted by the term 0(n).


  The number of anonymous functions in an expresion is understood to be equal to
  the number of times the asymptotic notation appears. For example, in the
  expression:

           Sum ^n _i=1 O(i)

  there is only a single anonymous function ( a function of i). This expression
  is thus not the same as O(1) + O(2) + ... + O(n), which doesn't really have a
  clean interpretation.  [? pg 50]

  In some cases, asymptotic notation appears on the left-hand side of an
  equation, as in:

          2n^2 + 0(n) = 0(n^2) .

  We interpret such equations using the following rule:

  No matter how the anonymous functions are chosen on the left of the equal
  sign, there is a way to choose the anonymous functions on the right of the
  equal sign to make the equation valid. Thus, our example means that for any
  function

    f(n) E 0(n), there is some function g(n) E 0(n^2) such that 2n^2 + f(n) =
                 g(n) for all n.

  In other words, the right-hand side of an equation provides a coarser level of
  detail than the left-hand side.

  We can chain together a number of such relationships, as in

     2n^2 + 3n + 1 = 2n^2 + 0(n) = 0(n^2)


  The first equation says that there is some function f(n) E 0(n) such that 2n^2
  + 3n + 1 = 2n^2 + f(n) for all n. The second equation says that for any
  function g(n) E 0(n) (such as the f(n) just mentioned), there is some function
  h(n) E 0(n^2) such that 2n^2 + g(n) = h(n) for all n. Note that this
  interpretation implies that 2n^2 + 3n + 1 = 0(n^2), which is what the chaining
  of equations intuitively gives us.


- o-notation

  The asymptotic upper bound provided by O-notation may or may not be
  asymptotically tight. The bound 2n^2 = O(n^2) is asymptotically tight, but the
  bound 2n = O(n^2) is not. We use o-notation to denote an upper bound that is
  not asymptotically tight. We formally define o(g(n)) ("little-oh of g of n")
  as the set

     0(g(n)) = { f(n): for *any* positive constant c > 0, there exists a
                      constant n0 > 0 such that 0 <= f(n) < cg(n) for all n >=
                      n0 }


   For example, 2n = o(n^2) but 2n^2 != o(n^2)


*       The definitions of O-notation and o-notation are similar. The main
*       difference is that in f(n) = O(g(n)), the bound 0 <=  f(n) <= cg(n)
*       holds for some constant c > 0, but in f(n) = o(g(n)), the bound 0 <=
*       f(n) < cg(n) holds for *all* constants c > 0.


  Intuitively, in o-notation, the function f(n) becomes insignificant relative
  to g(n) as n approaches infinity; that is,

          lim   f(n) / g(n)   = 0
        n -> inf


  Some authors use this limit as a definition of the o-notation; the definition
  in this book also restricts the anonymous functions to be asymptotically
  nonnegative.


- ω-notation

  By analogy, ω-notation is to Ω-notation as o-notation is to O-notation. We use
  ω-notation to denote a lower bound that is not asymptotically tight. One way
  to define it is by:

      f(n) E  ω (g(n)) if and only if g(n) E o(f(n)).


  Formally, however, we define  ω(g(n)) ("little-omega of g of n") as the set:

                 ω(g(n)) = { f(n) : for any positive constanc c > 0, there
                            exists a constant n0 > 0 such that 0 <= cg(n) < f(n)
                            for all n >= n0 }

   For example, n^2/2 =  ω(n), but n^2/2 !=  ω(n^2). The relation f(n) =
   ω(g(n)) implies that:

          lim             f(n)/g(n) = inf
        n -> inf

   if the limit exists. That is, f(n) becomes arbitrarily large relative to g(n)
   as n approaches infinity.


- Comparing functions

  Many of the relational properties of real numbers apply to asymptotic
  comparisons as well. For the following, assume that f(n) and g(n) are
  asymptotically positive.

  Transitivity:

        f(n) = 0(g(n)) and g(n) = 0(h(n)) implies f(n) = 0(h(n))

        f(n) = O(g(n)) and g(n) = O(h(n)) implies f(n) = O(h(n))

        f(n) = Ω(g(n)) and g(n) = Ω(h(n)) implies f(n) = Ω(h(n))

        f(n) = o(g(n)) and g(n) = o(h(n)) implies f(n) = o(h(n))

        f(n) = ω(g(n)) and g(n) = ω(h(n)) implies f(n) = ω(h(n))

  Reflexivity:

        f(n) = 0(f(n))
        f(n) = O(f(n))
        f(n) = Ω(f(n))


  Symmetry:

        f(n) = 0(g(n)) if and only if g(n) = 0(f(n)).


  Transpose symmetry:

        f(n) = O(g(n)) if and only if g(n) = Ω(f(n))

        f(n) = o(g(n)) if and only if g(n) = ω(f(n))



  Because these properties hold for asymptotic notations, we can draw an analogy
  between the asymptotic comparison of two functions f and g and the comparison
  of two real numbers a and b:


        f(n) = O(g(n)) is like a <= b

        f(n) = Ω(g(n)) is like a >= b

        f(n) = 0(g(n)) is like a = b

        f(n) = o(g(n)) is like a < b

        f(n) = ω(g(n)) is like a > b


  We say that f(n) is asymptotically smaller than g(n) if f(n) = o(g(n)), and
  f(n) is asymptotically larger than g(n) if f(n) = ω(g(n)).

  One property of real numbers, however, does not carry over to asymptotic
  notation:


  Trichotomy:

        For any two real numbers a and b, exactly one of the following must
        hold:

                        a < b, a = b, a > b


        Although any two real numbers can be compared, not all functions are
        asymptotically comparable. That is, for two functions f(n) and g(n), it
        may be the case that neither f(n) = O(g(n)) nor f(n) =  Ω(g(n)) holds.

        For example, we cannot compare the functions n and n^(1 + sin n) using
        asymptotic notation, since the value of the exponent in n^(1 + sin n)
        oscillates between 0 and 2, taking on all values in between.

- Standard notations and common functions

  - Monotonicity

        A function f(n) is monotonically increasing if m <= n implies f(m) <=
        f(n).  Similarly, it is monotonically decreasing if m <=n implies 
        f(m) >= f(n).

        A function f (n) is strictly increaing if m < n implies f(m) < f(n) and
        strictly decreasing if m < n implies f(m) > f(n)

  - Floors and ceilings

    For any real number x, we denote the greatest integer less or equal to x by
    |_ x _| (read "the floor of x") and the least integer greater than or equal
    to x by |~ x ~| (read "the ceiling of x"). For all real x,

        x - 1 < |_ x _|  <= x <= |~ x ~| < x + 1

    For any integer n,

        |~ n/2 ~| + |_ n/2 _| = n

    and for any real number x >= 0 and integers a, b > 0,


        |~        ~|
        |  |~x/a~| |        |~       ~|
        |  ------- |  =     |   x     |
        |    b     |        |   -     |
        |          |        |   ab    |


        [ same with floor ]


        |~   a  ~|          a + ( b - 1)
        |    -   |   < =    ------------
        |    b   |              b

        |    a   |          a + ( b - 1)
        |    -   |   > =    ------------
        |_   b  _|              b




  The floor function f(x) = |_ x _| is monotonically increasing, as is the
  ceiling function f(x) = |~ x ~|


- Modular arithmetic

  For any integer "a" and any positive integer "n", the value "a" mod "n" is the
  remainder (or residue) of the quotient a/n

     a mod n = a - n |_ a / n _|

  It follows that:

     0 <= a mod n < n

     [ where (=) is triple = sign ]

  If (a mod n) = (b mod n), we write a (=) b (mod n) and say that a is
  *equivalent* to b, modulo n.

  In other words, a (=) b (mod n) if and only if n is a divisor or b - a. We
  write a !(=) b (mod n) if a is not equivalent to b, modulo n.


- Polynomials

  Given a nonnegative integer d, a polynomial in "n" of degree "d" is a function
  p(n) of the form

        p(n) = Sum ^d _ i = 0   a_i n ^i

  where the constants a_0, a_1,...,a_d are the coefficients of the polynomial
  and a_d != 0.

  A polynomial is asymptotically positive if and only if a_d > 0.

  For an asymptotically positive polynomial p(n) of degree d, we have p(n) =
  0(n^d). For any real constant a >= 0, the function n^a is monotonically
  increasing, and for any real constant a <=0, the function n^a is monotonically
  decreasing. We say that a function f(n) is polynomially bounded if f(n) =
  O(n^k) for some constant k.


- Exponentials

  For all n and a >= 1, the function a^n is monotonically increasing in n. When
  convenient, we shall assume 0^0 = 1

  We can relate the rates of growth of polynomial and exponentials by the
  following fact. For all real constants a and b such that a > 1,


          lim      n^b / a^n  = 0
        n -> inf


  from which we can conclude that

           n^b = o(a^n)

  Thus, any exponential function with a base strictly greater than 1 grows
  faster than any polynomial function.


  Using e to denote 2.71828..., the base of the natural logarithm function, we
  have for all real x,

         e^x = 1 + x + x^2/2! + ...


  where "!" denotes the factorial function defined later in this section. For
  all real x, we have the inequality:

         e^x >= 1 + x

  where equality holds only when x = 0. when |x| <= 1, we have the apporimation

         1 + x <= e^x <= 1 + x + x^2

  When x -> 0, the approximation of e^x by 1 + x is quite good:

         e^x = 1 + x + 0(x^2)

  (In this equation,  the asymptotic notation is used to describe the limiting
  behavior as x-> 0 rather than as x -> inf) We have for all x,

         lim   ( 1 + x/n) ^ n   = e^x
      n -> inf

- Logarithms

  We shall use the following notations:

      lg n = log_2 n

      lg lg n = lg(lg n)

  An important notational convention we shall adopt is that logarithm functions
  will apply only to the next term in the formula, so that lg n + k will mean
  (lg n) + k and not lg(n + k). If we hold b > 1 constant, then for n > 0, the
  function log_b n is strictly increasing.

        lg_b a = x
         b ^ x =  a
         log_c (b^x) = log_c (a)
         x  log_c (b) = log_c (a)
         x = log_c(a) / log_c(b) = lg_b a

         (3.15)

  By equation (3.15), changing the base of a logarithm from one constant to
  another changes the value of the logarithm by only a constant factor, and so
  we shall often use the notation "lg n" when we don't care about constant
  factors, such as in O-notation. Computer scientists find 2 to be the most
  natural base for logarithms because so many algorithms and data structures
  involve splitting a problem into two parts.

  There is a simple series expansion for ln(1 + x) when |x| < 1:

         ln(1+x) = x - x^2/2 + x^3/3 - x^4/4 + ...


  We also have the following inequalities for x > -1:

         x/ ( 1+x) <= ln (1+x) <= x

  Where equality holds only for x = 0

  We say that a function f(n) is polylogarithmically bounded if f(n) = O(lg^k n)
  for some constant k. We can relate the growth of polynomials and
  polylogarithms by substituting lg n for n and 2^a for a in equation (3.10),
  yielding


        lim     lg^b n       = lim    lg^b n     = 0
     n -> inf   ------      n -> inf  ------
              (2^a)^lg n                n^a

     From this limit, we can conclude that

                lg^b n = o(n^a)

     for any constant a > 0. Thus, any positive polynomial function grows faster
     than any polylogarithmic function.


- Factorials

  A weak upper bound on the factorial function is n! <= n^n, since each of the n
  terms in the factorial product is at most n. Stirling's approximation,

      n! = (2 pi n)^ 0.5  (n/e) ^ n ( 1 + 0(1/n))

  The following equation also holds for all n >= 1:

      n! = (2 pi n)^0.5 (n/e)^n e^(alpha * n)

   where

             1   < alpha_n   <     1
        --------                ------
        12n + 1                   12n


- Functional iteration

  We use the notation f^(i) (n) to denote the function f(n) iteratively applied
  "i" times to an initial value of n. Formally, let f(n) be a function over the
  reals. For non-negative integers i, we recursively define


      f^(i) (n)    =    n            if i = 0
                    f(f^(i-1) (n))   if i > 0

    For example, if f(n) = 2n, then f^(i) (n) = 2^i n

- The iterated logarithm function

   We use the notation lg * n ("log star of n") to denote the iterated
   logarithm, defined as follows.

    Let lg^(i) n be as defined above, with f(n) = lg n. Because the logarithm of
    a nonpositive number is undefined, lg^(i) n is defined only if lg^(i-1) n >
    0. Be sure to distinguish lg^(i) n (the logarithm function applied i times
    in succession, starting with argument n) from lg^i n (the logarithm of n
    raised to the i_th power). Then we define the iterated logarithm function as

        lg * n = min { i >= 0:lg^(i) n <= 1 }

        f^(i) (n) = lg n,           if i = 0                ?
                  = f(f^(i-1)) (n)  if i > 0                ?

      The iterated logarithm is a very slowly growing function:

      So the result is "i" ?

            lg* 2 = 1
            lg 2 = 1;  i = 1

            lg* 4 = 2
            lg 4 = 2, lg 2 = 1; i = 2

            lg* 16 = 3
            lg 16 = 4, lg 4 = 2, lg 2 = 1, i = 3

            lg* 65536 = 4
            lg 65536 = 16, lg 16 = 4, lg 4 = 2, lg 2 = 1, i =4

            lg* (x^65536) = 5

       Since the number of atoms in the observable universe is estimated to be
       about 10^80, which is much less than 2^65536, we rarely encounter an
       input size n such that lg* n > 5.

- Fibonacci numbers

  We define the Fibonacci numbers by the following recurrence:

                  F0 = 0
                  F1 = 1
                  Fi = Fi-1 + Fi-2      for i >= 2

   Thus, each Fibonacci number is the sum of the two previous ones, yielding the
   sequence


                 0; 1; 1; 2; 3; 5; 8; 13; 21; 34; 55;

   Fibonacci numbers are related to the golden ratio phi and to its conjugate,
   which are the roots of the equation

                 x^2 = x + 1

    and are given by the following formulas:

                 phi = (1 + sqrt(5)) / 2


                 phi_conjugate = (1- sqrt(5)) / 2

    Specifically, we have

              Fi = phi^i - phi_conjugate^ i
                   --------------------------
                          sqrt(5)

     which we can prove by induction [ proof not shown ]. Since |phi_conjugate|
     < 1, we have

           |phi_conjugate ^i|    <     1
          ------------------        ------
             sqrt ( 5)              sqrt(5)

                                 <    1
                                     ---
                                      2

       which implies that:

         Fi =  | phi^i         +         1   |
               |-------                 ---  |
               | sqrt(5)                 2   |
               |_                           _|


* Divide and Conquer pg 65

  Recall that in divide-and-conquer, we solve a problem recursively, applying
  three steps at each level of the recursion:

   [Divide] the problem into a number of subproblems that are smaller instances
            of the same problem

   [Conquer] the subproblems by solving them recursively. If the subproblem
             sizes are small enough, however, just solve the subproblems in a
             straightforward manner.

   [Combine] the solutions to the subproblems into the solution for the original
             problem.


  When the subproblems are large enough to solve recursively, we call that the
  recursive case.

  Once the subproblems become small enough that we no longer recurse, we say
  that the recursion "bottoms out" and that we have gotten down to the base
  case.

  Sometimes, in addition to subproblems that are smaller instances of the same
  problem, we have to solve subproblems that are not quite the same as the
  original problem. We consider solving such subproblems as part of teh combine
  step.

- Recurrences

  Recurrences go hand in hand with the divide-and-conquer paradigm, because they
  give us a natural way to characterize the running times of divide-and-conquer
  algorithms.

  A recurrence is an equation or inequality that describes a function in terms
  of its value on smaller inputs.

  e.g Merge-Sort

      T(n)            = 0(1)                  if n = 1
                        2T(n/2) + 0(n)        if n > 1

      whose solution we claimed to be T(n) = 0(n lg n)

  Recurrences can take many forms. For example, a recursive algorithm might
  divide subproblems into unequal sizes, such as 2/3 to 1/3 split. If the divide
  and combine steps take linear time, such an algorithm would give rise to the
  occurence

      T(n) = T(2n/3) + T(n/3) + 0(n)

  Subproblems are not necessarily constrained to being a constant fraction of
  the original problem size. For example, a recursive version of linear search
  would create just one subproblem containing only one element fewer than the
  original problem. Each recursive call would take constant time plus the time
  for the recursive calls it makes, yielding the recurrence

      T(n) = T(n - 1) + 0(n)


  This chapter offers three methods for solving recurrences - that is, for
  obtaining asymptotic "0" or "O" bounds on the solution:

   - In the substitution method, we guess a bound and then use mathematical
     induction to prove our guess correct

   - The recursion-tree method converts the recurrence into a tree whose nodes
     represent the costs incurred at varios levels of the recursion. We use
     techniques for bounding summations to solve the recurrence.

   - The master method provides bounds for recurrences of the form

       T(n) = a T(n/b) + f(n)

       where a >= 1, b > 1, and f(n) is a given function. Such recurrences arise
       frequently.

      A recurrence of the form ^^ characterizes a divide-and-conquer algorithm
      that creates "a" subproblems, each of which is 1/b the size of the
      original problem, and in which the divide and combine steps together take
      f(n) time.


      To use the master method, you will need to memorize three cases, but once
      you do that, you will easily be able to determine asymptotic bounds for
      many simple recurrences. We will use the master method to determine the
      running times of the divide-and-conquer algorithms for the
      maximum-subarray problem and for matrix multiplication, as well as for
      other algorithms based on divide-and-conquer elsewhere in this book.

   Occasionally, we shall see recurrences that are not equalities but rather
   inequalities, such as  T(n) <= 2 T(n/2) + 0(n).  Because such a recurrence
   states only an upper bound on T(n), we will couch its solution using
   O-notation rather than 0-notation. Similarly, if the inequality were reversed
   to T(n) >= 2T(n/2) + 0(n), then because the recurrence gives only a lower
   bound on T(n), we would use Ω-notation in its solution.


- Technicalities in recurrences

  In practice, we neglect certain technical details when we state and solve
  recurrences. For example, if we call MergeSort on "n" elements when n is odd,
  we end up with subproblems of size |_n/2_| and |~n/2~|. Neither size is
  actually n/2, because n/2 is not an integer when n is odd. Technically, the
  recurrence describing the worst-case running time of MergeSort is really

     T(n) = 0(1)                               if n = 1
            T(|~n/2~| + T(|_n/2_| + 0(n)       if n > 1

  Boundary conditions represent another class of details that we typically
  ignore. Since the running time of an algorithm on a constant-sized input is a
  constant, the recurrences that arise from the running times of algorithms
  generally have T(n) = 0(1) for sufficiently small n. Consequently, for
  convenience, we shall generally omit statements of the boundary conditions of
  recurrences and assume that T(n) is constant for small n. For example, we
  normally state recurrence as:

     T(n) = 2T(n/2) + 0(n)

  without explicitly giving values for small n. The reason is that although
  changing the value of T(1) changes the exact solution to the recurrene, the
  solution typically doesn't change by more that a constant factor, and so the
  order of growth is unchanged.

  When we state and solve recurrences, we often omit floors, ceilings, and
  boundary conditions. We forge ahead without these details and later determine
  whether or not they matter. They usually do not, but you should know when they
  do.

  Experience helps, and so do some theorems stating that these details do not
  affect the asymptotic bounds of many recurrences characterizing
  divide-and-conquer algorithms. In this chapter, however, we shall address some
  of these details and illustrate the fine points of recurrence solution
  methods.


- The maximum-subarray problem

  [About how to buy stock: first approach is to check daily prices. Second
  approach is to look the daily change in price, where the change on day i is
  the difference between the prices after day i - 1 and after day i. ]

  If we treat the row of changes as an array A, we now want to find the
  nonempty, contiguous subarray of A whose values have the largest sum. We call
  this contiguous subarray the maximum subarray.

  For example in the array of Figure 4.3 [pg 69], the maximum subarray of
  A[1..16] is A[8..11], with the sum 4. Thus, you would want to buy the stock
  just before day 8 (that is, after day 7) and sell it after day 11, earning a
  profit of $43 per share.

  At first glance, this transformation does not help. We still need to check 

  |n - 1|  = 0(n^2) subarays for a period of n days.
  |  2  |

   [...]

   So let us seek a more efficient solution to the maximum-subarray problem.
   When doing so, we will usually speak of “a” maximum subarray rather than
   “the” maximum subarray, since there could be more than one subarray that
   achieves the maximum sum.

   The maximum-subarray problem is interesting only when the array contains some
   negative numbers. If all the array entries were nonnegative, then the
   maximum-subarray problem would present no challenge, since the entire array
   would give the greatest sum.

   - A solution using divide-and-conquer

     Let’s think about how we might solve the maximum-subarray problem using the
     divide-and-conquer technique. Suppose we want to find a maximum subarray
     of the subarray A[low..high]. Divide-an-conquer suggests that we divide
     the subarray into two subarrays of as equal size as possible. That is, we
     find the midpoint, say mid, of the subarray, and consider the subarrays
     A[low..mid] and A[mid + 1..high]. Any contiguous subarray A[i..j] of
     A[low..high] must lie in exactly one of the following places:

     - entirely in the subarray A[low..mid], so that low <= i <= j <= mid
     - entirely in the subarray A[mid+1..high], so that mid <= i <= j <= high,
       or
     - crossing the midpoint, so that low <= i <= mid < j <= high

     Therefore, a maximum of A[low..high] must lie in exactly one of these
     places.

     We can find maximum subarrays of A[low..mid] and A[mid + 1..high]
     recursively, because these two subproblems are smaller instances of the
     problem of finding a maximum subarray.

     Thus, all that is left to do is find a maximum subarray that crosses the
     midpoint, and take a subarray with the largest sum of the three.

     We can easily find a maximum subarray crossing the midpoint in time linear
     in the size of the subarray A[low..high]. This problem is not a smaller
     instance of our original problem, because it has the added restriction that
     the subarray it chooses must cross the midpoint.

     Any subarray crossing the midpoint is itself made of two subarrays
     A[i..mid] and A[mid+1..j], where low <= i <= mid and mid < j <= high.
     Therefore, we just need to find maximum subarrays of the form A[i..mid] and
     A[mid + 1 .. j] and then combine them.

     The procedure Find-Max-Crossing-Subarray takes as input the array A and the
     indices low, mid, and high, and it returns a tuple containing the indices
     demarcating a maximum subarray that crosses the midpoint, along with the
     sum of the values in a maximum subarray.

          Find-Max-Crossing-Subarray (A, low, mid, high)

            1 left-sum = - inf
            2 sum = 0
            3 for i = mid downto low
            4       sum = sum + A[i]
            5   if sum > left-sum
            6       left-sum = sum
            7       max-left = i
            8 right-sum = -inf
            9 sum = 0
           10 for j = mid + 1 to high
           11   sum = sum + A[j]
           12   if sum > right-sum
           13       right-sum = sum
           14       max-right = j
           15 return (max-left, max-right, left-sum + right-sum)


    If the subarray A[low..high] contains n entries ( so that n = high - low +
    1), we claim that the call Find-Max-Crossing-Subarray (A, low, mid, high)
    takes 0(n) time. Since each iteration of each of the two for loops takes
    0(1) time, we just need to count up how many iterations there are
    altogether. The for loop of lines 3-7 makes mid - low + 1 iterations, and
    the for loop of lines 10-14 makes high-mid iterations, and so the total
    number of iterations is:

         (mid - low + 1) + (high - mid)  = high - low + 1
                                         = n

    With a linear-time FIND-MAX-CROSSING-SUBARRAY procedure in hand, we
    can write pseudocode for a divide-and-conquer algorithm to solve the
    maximum-subarray problem:

            FIND-MAXIMUM-SUBARRAY (A, low, high)
            1   if high == low
            2       return (low, high, A[low]) // base case: only one element

            3   else mid = (low + high) / 2
            4       (left-low, left-high, left-sum) =
                        FIND-MAXIMUM-SUBARRAY(A, low, mid)

            5       (right-low, right-high, right-sum) =
                        FIND-MAXIMUM-SUBARRAY(A, mid + 1, high)

            6       (cross-low, cross-high, cross-sum) =
                        FIND-MAX-CROSSING-SUBARRAY (A, low, mid, high)

            7   if left-sum >= right-sum and left-sum >= cross-sum
            8       return (left-low, left-high, left-sum)

            9   elseif right-sum >= left-sum and right-sum >= cross-sum
           10       return (right-low, right-high, right-sum)

           11   else return (cross-low, cross-high, cross-sum)


  e.g      [1 ,2 ,3 ,4 ,5, 6, 7, 8]

           [1, 2, 3, 4]    [5, 6, 7, 8]

        [1, 2] [3, 4]      [5, 6] [7, 8]

      [1] [2] [3] [4]    [5] [6] [7] [8]


  e.g      [-5, 7, -15, 8, 20, 60, -60, 9]


         [-5, 7, -15, 8]    [20, 60, -60, 9]

        [-5, 7]  [-15, 8]  [20 ,60]  [-60, 9]

        [-5] [7] [-15] [8] [20] [60] [-60] [9]


  - Analyzing the divide-and-conquer algorithm

    We denote by T(n) the running time of FIND-MAXIMUM-SUBARRAY procedure on a
    subarray of n elements.

    Line 1 takes constant time.
    Line 2 takes constant time.

        n = 1 ==> T(1) = 0(1)

    The recursive case occurs when n > 1.

    Each of the subproblems solved in lines 4 and 5 is ona subarray of n/2
    elements, and so we spend T(n/2) time solving each of them.

     ==> 2 T(n/2)

     Calling FIND-MAX-CROSSING-SUBARRAY takes 0(n) times.

     Lines 7-11 take only 0(1) time.


        T(n) = 0(1) + 2 T(n/2) + 0(n) + 0(1)

             = 2 T(n/2) + 0(n)


        T(n) = 0(1)                 if n = 1
               2 T(n/2) + 0(n)      if n > 1

     This recurrence is the same as merge sort.

        Hence, T(n) = 0(n lg n)  <-- from the master method in Sec 4.5

    Thus, we see that the divide-and-conquer method yields an algorithm that is
    asymptotically faster than the brute-force method. With merge sort and now
    the maximum-subarray problem, we begin to get an idea of how powerful the
    divide- and-conquer method can be. Sometimes it will yield the
    asymptotically fastest algorithm for a problem, and other times we can do
    even better. As Exercise 4.1-5 shows, there is in fact a linear-time
    algorithm for the maximum-subarray problem, and it does not use
    divide-and-conquer.


    e.x 4.1-5


    Use the following ideas to develop a nonrecursive, linear-time algorithm for
    the maximum-subarray problem. Start at the left end of the array, and
    progress toward the right, keeping track of the maximum subarray seen so
    far. Knowing a maximum subarray of A[1..j], extend the answer to find
    a maximum subarray ending at index j + 1 by using the following
    observation: a maximum subarray of A[1..j + 1] is either a maximum
    subarray of A[1..j] or a subarray A[1..j + 1], for some 1 <= i <= j + 1
    Determine a maximum subarray of the form A[1..j + 1] in
    constant time based on knowing a maximum subarray ending at index j .
    (?)


- Strassen's algorithm for matrix multiplication

  If you have seen matrices before, then you probably know how to multiply them.

  If A = (a_ij) and B = (bij) are square n x n matrices, then in the product

  C = A · B, we define the entry c_ij, for i,j = 1,2,...,,n, by

    c_ij = Sum^n _k=1   a_ik · b_kj

  We must compute n^2 matrix entries, and each is the sum of n values.o

    SQUARE-MATRIX-MULTIPLY(A, B)

    1   n = A.rows
    2   let C be a new n x n matrix
    3   for i = 1 to n
    4       for j = 1 to n
    5           c_ij = 0
    6           for k = 1 to n
    7              c_ij = c_ij + a_ik · b_kj

    8   return C

  Because each of the triply-nested for loops runs exactly n iterations, and
  each execution of line 7 takes constant time, the SQUARE-MATRIX-MULTIPLY
  procedure takes 0(n^3) time.

  You might at first think that any matrix multiplication algorithm must take
  Ω(n^3) time, since the natural definition of matrix multiplication requires
  that many multiplications. You would be incorrect, however: we have a way to
  multiply matrices in o(n^3) time. In this section, we shall see Strassen’s
  remarkable recursive algorithm for multiplying n x n matrices. It runs in
  O(n ^ lg 7) time, which we shall show in Section 4.5. Since lg 7 lies between
  2:80 and 2:81, Strassen’s algorithm runs in O(n ^2:81) time, which is
  asymptotically better than the simple SQUARE-MATRIX-MULTIPLY procedure.

  - A simple divide-and-conquer algorithm

    To keep things simple, when we use a divide-and-conquer algorithm to
    compute the matrix product C = A · B, we assume that n is an exact power of
    2 in each of the n x n matrices. We make this assumption because in each
    divide step, we will divide n x n matrices into four n/2 x n/2 matrices, and
    by assuming that n is an exact power of 2, we are guaranteed that as long as
    n >= 2, the dimension n/2 is an integer.


    Suppose that we partition each of A, B, and C into four n/2 x n/2 matrices

        A = | A_11   A_12 |
            | A_21   A_22 |

        B = | B_11   B_12 |
            | B_21   B_22 |

        C = | C_11   C_12 |
            | C_21   C_22 |             (eqn 4.9)

    so that we rewrite the equation C = A · B as

      C_11 = A_11 · B_11 + A_12 · B_21
      C_12 = A_11 · B_12 + A_12 · B_22
      C_21 = A_21 · B_11 + A_12 · B_21
      C_22 = A_21 · B_12 + A_12 · B_22


    Each of these four equations specifies two multiplications of n/2 x n/2
    matrices and the addition of their n/2 x n/2 products. We can use these
    equations to create a straightforward, recursive, divide-and-conquer
    algorithm:


        SQUARE-MATRIX-MULTIPLY-RECURSIVE(A, B)

        1   n = A.rows
        2   let C be a new n x n matrix
        3   if n == 1
        4       c_11 = a_11 · b_11
        5   else partition A, B, and C as is eqn 4.9
        6       C_11 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_11, B_11)
                        + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_12, B_21)

        7       C_12 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_11, B_12)
                        + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_12, B_22)

        8       C_21 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_21, B_11)
                        + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_22, B_21)

        9       C_22 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_21, B_12)
                        + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_22, B_22)

       10       return C

    This pseudocode glosses over one subtle but important implementation detail.
    How do we partition the matrices in line 5? If we were to create 12 new n/2
    x n/2 matrices, we would spend 0(n^2) time copying entries. In fact, we can
    parition the matrices without copying entries. The trick is to use index
    calculations. We identify a submatrix by a range of row indices and a range
    of column indices of the original matrix.

    We end up representing a submatrix a little differently from how we
    represent the original matrix, which is the subtlety we are glossing over.
    The advantage is that, since we can specify submatrices by index
    calculations, executing line 5 takes only 0(1) time (although we shall see
    that it makes no difference asymptotically to the overall running time
    whether we copy or partition in place).

    Now, we derive a recurrence to characterize the running time of
    SQUARE-MATRIX-MULTIPLY-RECURSIVE. Let T(n) be the time to multiply two n x n
    matrices using this procedure. In the base case, when n = 1, we perform just
    the one scalar multiplicaiton in line 4, and so

        T(1) = 0(1)

    The recursive case occurs when n > 1. As discussed, partitioning the
    matrices in line 5 takes 0(1) time, using index calculations. In lines 6-9,
    we recursively call SQUARE-MATRIX-MULTIPLY-RECURSIVE a total of eight times.
    Because each recursive call multiplies two n/2 x n/2 matrices, thereby
    contributing T(n/2) to the overall running time, the time taken by all eight
    recursive calls is 8 T(n/2). We also must account for the four matrix
    additions in lines 6-9. Each of these matrices contains n^2 / 4 entries, and
    so each of the four matrix additions takes 0(n^2) time [think of how you
    would implmemnt addition: for i in row; for j in column, hence n^2]. 

    Since the number of matrix additions is a constant, the total time spent
    adding matrices in lines 6-9 is 0(n^2). (Again, we use index calculations to
    place the results of the matrix additions into the correct positions of
    matrix C, with an overhead of 0(1) time per entry.) The total time for
    recursive case, therefore, is the sum of the partitioning time, the time for
    all the recursive calls, and the time to add the matrices resulting from the
    recursive calls:

        T(n) = 0(1) + 8 T(n/2) + 0 (n^2)
             = 8 T(n/2) + 0(n^2)

    Notice that if we implemented partitioning by copying matrices, which would
    cost 0(n^2) time, the recurrence would not change, and hence the overall
    running time would increase by only a constant factor.

    Combining equations gives us the recurrence for the running time of
    SQUARE-MATRIX-MULTIPLY-RECURSIVE:

        T(n) = 0(1)                 if n = 1
               8 T(n/2) + 0(n^2)    if n > 1

    As we shall see from the master method, recurrence has the solution T(n) =
    0(n^3). Thus, this simple divide-and-conquer approach is no faster than the
    straightforward SQUARE-MATRIX-MULTIPLY procedure.


  - Strassen's method

    The key to Strassen's method is to make the recursion tree slightly less
    bushy. That is, instead of performing eight recursive multiplications of n/2
    x n/2 matrices, it performs only seven. The cost of eliminating one matrix
    multiplication will be several additions of n/2 x n/2 matrices, but still
    only a constant number of additions. As before, the constant number of
    matrix additions will be subsumed by 0-notation when we set up the
    recurrence equation to characterize the running time.

    Strassen's method is not at all obvious. It has four steps:

    1. Divide the input matrices A and B and output matrix C into n/2 x n/2
    submatrices, as in eqn 4.9. This step takes 0(1) time by index calculation,
    just as in SQUARE-MATRIX-MULTIPLY-RECURSIVE time by index calculation, just
    as in SQUARE-MATRIX-MULTIPLY-RECURSIVE.

    2. Create 10 matrices S1, S2, ..., S10, each of which is n/2 x n/2 and is
    the sum or difference of two matrices created in step 1. We can create all
    10 matrices in 0(n^2) time.

    3. Using the submatrices created in step1 and the 10 matrices created in
    step2, recursively compute seven matrix products P1, P2, ..., P7. Each
    matrix Pi is n/2 x n/2.

    4. Compute the desired submatrices C11, C12, C21, C22 of the result matrix C
    by adding and subtracting various combinations of the Pi matrices. We can
    compute all four submatrices in 0(n^2) time.


    Hence,

        T(n) = 0(1)                 if n = 1
               7 T(n/2) + 0(n^2)    if n > 1

    We have traded off one matrix multiplication for a constant number of matrix
    additions. Once we understand recurrences and their solutions, we shall
    see that this tradeoff actually leads to a lower asymptotic running time.

    By the master method, solution is T(n) = 0(n^ lg 7).



    In step2, we create the following 10 matrices:

    S1 = B12 - B22
    S2 = A11 + A12
    S3 = A21 + A22
    D4 = B21 - B11
    S5 = A11 + A22
    S6 = B11 + B22
    S7 = A12 - A22
    S8 = B21 + B22
    S9 = A11 - A21
    S10 = B11 + B12

  Since we must add or subtract n/2 x n/2 matrices 10 times, this step does
  indeed take 0(n^2) time.


  In step 3, we recursively multiply n/2 x n/2 matrices seven times to compute
  the following n/2 x n/2 matrices, each of which is the sum or diference of
  products of A and B submatrices:

    P1 = A11 · S1
    P2 = S2 · B22
    P3 = S3 · B11
    P4 = A22 · S4
    P5 = S5 · S6
    P6 = S7 · S8
    P7 = S9 · S10

  [ 7 times apply function recursively, (vs 8) ]
  Note that the only multiplications we need to perform are those in the middle
  column of the above equations. 


  Step 4 adds and subtracts the Pi matrices created in step 3 to construct the
  four n/2 x n/2 submatrices of the product C. We start with:

    C11 = P5 + P4 - P2 + P6

    [if you expand it, you'll get the usual command]

    C12 = P1 + P2

    C21 = P3 + P4

    C22 = P5 + P1 - P3 - P7

    This take 0(n^2) time.




- The substitution method for solving recurrences

  The substitution method for solving recurrences comprises two steps:

  1. Guess the form of the solution

  2. Use mathematical induction to find the constants and show that the solution
  works.

  We substitute the guessed solution for the function when applying the
  inductive hypothesis to smaller values; hence the name “substitution method.”
  This method is powerful, but we must be able to guess the form of the answer
  in order to apply it.  We can use the substitution method to establish either
  upper or lower bounds on a recurrence. As an example, let us determine an
  upper bound on the recurrence

    T(n) = 2 T( |_ n/2 _| ) + n

  We guess that the solution  is T(n) = O( n lg n ). The substitution method
  requires us to prove that T(n) <= cn lg n for an appropriate choice of the
  constant c > 0. We start by assuming that this bound holds for all positive m
  < n, in particular for m = |_ n/2 _|, yielding 

    T(|_ n/2 _|) <= c |_n/2_| lg(|_n/2_|)

  Substituting into the recurrence yields

    T(n) <= 2(c |_n/2_| lg(|_n/2_|)) + n
         <= cn lg (n/2) + n
          = cn lg n - cn lg 2 + n
          = cn lg n - cn + n
         <= cn lg n

  where the last step holds as long as c >= 1.

  Mathematical induction now requires us to show that our solution holds for the
  boundary conditions. Typically, we do so by showing that the boundary
  conditions are suitable as base cases for the inductive proof. For the
  recurrence 4.19, we must show that we can choose the constant c large enough
  so that the bound T(n) <= cn lg n works for the boundary conditions as well.
  This requirement can sometimes lead to problems. Let us assume, for the sake
  of argument, that T(1) = 1 is the sole boundary condition of the recurrence.
  Then for n = 1, the bound T(n) <= cn lg n yields T(1) <= c1 lg 1 = 0, which is
  at odds with T(1) = 1. Consequently, the base case of our inductive proof
  fails to hold.

  We can overcome this obstacle in proving an inductive hypothesis for a
  specific boundary condition with only a little more effort. In the recurrence
  4.19, for example, we take advantage of asymptotic notation requiring us only
  to prove T(n) <= cn lg n for n >= n0, where n0 is a constant that we get to
  choose. We keep the troublesome boundary condition T(1) = 1, but remove it
  from consideration in the inductive proof. We do so by first observing that
  for n > 3, the recurrence does not depend directly on T(1). Thus, we can
  replace T(1) by T(2) and T(3) as the base cases in the inductive proof,
  letting n0 = 2.


  Note that we make a distinction between the base case of the recurrence (n =
  1) and the base cases of the inductive proof (n = 2 and n = 3). With 
  T(1) = 1, we derive from the recurrence that T(2) = 4, and T(3) = 5.

  Now we can complete the inductive proof that T(n) <= cn lg n for some constant
  c >= 1 by choosing c large enough so that T(2) <= c2lg 2 and
  T(3) <= c3lg 3. 

  As it turns out, any choice of c >= 2 suffices for the base cases of n = 2 and
  n = 3 to hold. For most of the recurrences we shall examine, it is
  straightforward to extend boundary conditions to make the inductive assumption
  work for small n, and we shall not always explicitly work out the details.


- Making a good guess

  Unfortunately, there is no general way to guess the correct solutions to
  recurrences.  Guessing a solution takes experience and, occasionally,
  creativity.  Fortunately, though, you can use some heuristics to help you
  become a good guesser. You can also use recursion trees, which we shall see in
  Section 4.4, to generate good guesses.

  If a recurrence is similar to one you have seen before, then guessing a
  similar solution is reasonable. As an example, consider the recurrence

    T(n) = 2T (|_ n/2_| + 17) + n

  which looks difficult because of the added “17” in the argument to T on the
  right-hand side. Intuitively, however, this additional term cannot
  substantially affect the solution to the recurrence.

  When n is large, the difference between |_n/2_| and |_n/2_| + 17 is not that
  large: both cut n nearly evenly in half. Consequently, we make the guess that
  T(n) = O(n lg n), which you can verify as correct by using the substitution
  method.

  Another way to make a good guess is to prove loose upper and lower bounds on
  the recurrence and then reduce the range of uncertainty.


- Subtleties

  Sometimes you might correctly guess an asymptotic bound on the solution of a
  recurrence, but somehow the math fails to work out in the induction. The
  problem frequently turns out to be that the inductive assumption is not strong
  enough to prove the detailed bound. If you revise the guess by subtracting a
  lower-order term when you hit such a snag, the math often goes through.

  Consider the recurrece:

    T(n) = T(|_n/2_|) + T(|~ n/2 ~|) + 1

  We guess that the solution is T(n) = O(n), and we try to show that T(n) <= cn
  for an appropriate choice of the constant c. Submitting our guess in the
  recurrence, we obtain

    T(n) <= c |_n/2_| + c|~ n/2 ~| + 1
          = cn + 1

  which does not imply T(n) <= cn for any choice of c. We might be tempted to
  try a larger guess, say T(n) = O(n^2). Although we can make this larger guess
  work, our original guess of T(n) = O(n) is correct. In order to show that it
  is correct, however, we must take a stronger inductive hypothesis.

  Intuitively, our guess is nearly right: we are off only by the constant 1, a
  lower-order term. Nevertheless, mathematical induction does not work unless we
  prove the exact form of the inductive hypothesis. We overcome our difficulty
  by subtracting a lower-order term from our previous guess.

  Our new guess is T(n) <= cn - d, where d >= 0 is a constant. We now have:

    T(n) <= (c|_n/2_| - d) + (c|~ n/2 ~| - d) + 1
          = cn - 2d + 1
         <= cn - d

  as long as d >= 1. As before, we must choose the constant c large enough to
  handle the boundary conditions.

  You might find the idea of subtracting a lower-order term counterintuitive.
  After all, if the math does not work out, we should increase our guess,
  right?  Not necessarily! When proving an upper bound by induction, it may
  actually be more difficult to prove that a weaker upper bound holds, because
  in order to prove the weaker bound, we must use the same weaker bound
  inductively in the proof.  In our current example, when the recurrence has
  more than one recursive term, we get to subtract out the lower-order term of
  the proposed bound once per recursive term.

- Avoiding pitfalls

  It is easy to err in the use of asymptotic notation. For example, in the
  recurrence 4.19 we can falsely prove T(n) = O(n) by guessing T(n) <= cn and
  then arguing

    T(n) <= 2(C|_ n/2 _|) + n
         <= cn + n
          = O(n)         <===== wrong

  since c is a constant. The error is that we have not proved the exact form of
  the inductive hypothesis, that is, that T(n) <= cn. We therefore will
  explicitly prove that T(n) <= cn where we want to show that T(n) = O(n).


- Changing variables

  Sometimes, a little algebraic manipulation can make an unknown recurrence
  similar to one you have seen before. As an example, consider the recurrence

    T(n) = 2T(|_ sqrt(n) _|) + lg n

  which looks difficult.

  Renaming m = lg n yields

    T(2^m) = 2T(2^(m/2)) + m

  We can now rename S(m) = 2S(n/2) + m

    S(m) = O(m lg m)

    T(n) = T(2^m) = S(m) = O(m lg m) = O(lg n lg lg n)



- The recursion-tree method for solving recurrences

  Although you can use the substitution method to provide a succinct proof that
  a solution to a recurrence is correct, you might have trouble coming up with a
  good guess. Drawing out a recursion tree, as we did in our analysis of the
  merge sort recurrence in Section 2.3.2, serves as a straightforward way to
  devise a good guess.

  In a recursion tree, each node represents the cost of a single subproblem
  somewhere in the set of recursive function invocations. We sum the costs
  within each level of the tree to obtain a set of per-level costs, and then we
  sum all the per-level costs to determine the total cost of all levels of the
  recursion.

  A recursion tree is best used to generate a good guess, which you can then
  verify by the substitution method. When using a recursion tree to generate a
  good guess, you can often tolerate a small amount of “sloppiness,” since you
  will be verifying your guess later on. If you are very careful when drawing
  out a recursion tree and summing the costs, however, you can use a recursion
  tree as a direct proof of a solution to a recurrence.

  In this section, we will use recursion trees to generate good guesses, and in
  Section 4.6, we will use recursion trees directly to prove the theorem that
  forms the basis of the master method.

  For example, let us see how a recursion tree would provide a good guess for
  the recurrece T(n) = 3T( |_ n/4 _| ) + 0(n^2). We start by focusing on finding
  an upper bound for the solution. Because we know that floors and ceilings
  usually do not matter when solving recurrences (here's an example of
  sloppiness that we can tolerate), we create a recursion tree for the
  recurrence T(n) = 3T(n/4) + cn^2, having written out the implied constant
  coefficient c > 0.

  For convenience, we assume that n is an exact power of 4 (another example of
  tolerable sloppiness) so that all subproblem sizes are integers.

  The cn^2 term at the root represent the costs incurred by the subproblems of
  size n/4.


                        cn^2

                T(n/4)  T(n/4)  T(n/4)





                        cn^2

                c(n/4)^2   c(n/4)^2      c(n/4)^2


            T(n/16)     ·    ·    ·







                        cn^2                                             cn^2
^
            c(n/4)^2   c(n/4)^2      c(n/4)^2                       (3/16)cn^2
log_4 n

              ·            ·            ·                          (3/16)^2 cn^2
.
.
        T(1)  ·  ·  ·                                             0(n^(log_4 3))



                                                        Total: O(n^2)


  Because subproblem sizes decrease by a factor of 4 each time we go down one
  level, we eventually must reach a boundary condition. How far from the root do
  we reach one?

  The subproblem size for a node at depth i is n / 4^i. Thus, the subproblem
  size hits n = 1 when n / 4^i = 1, or equivalently, when i = log_4 n.

  Thus, the tree has log_4 n + 1 levels (at depthhs 0,1,2,...,log_4 n)

  Next we determine the cost at each level of the tree. Each level has three
  times more nodes than the level above, and so the number of nodes at depth i
  is 3^i.

  Because subproblem sizes reduce by a factor of 4 for each level we go down
  from the root, each node at depth i, for i = 0, 1, 2, ..., log_4 n - 1 has a
  cost of c(n/4^i)^2.

  Multiplying, we see that the total cost over all nodes at depth i, for i = 0,
  1, 2, ..., log_4 n   - 1 is   (3^i)c (n / (4^i)^2) = (3/16)^i cn^2.

  The bottom level, at depth log_4 n , has 3^(log_4 n) = n^ (log_4 3) nodes,
  each contributing cost T(1, for a total cost of n^(log_4 3) T(1), which is
  0(n^(log_4 3)), since we assume that T(1) is a constant.



  T(n) = cn^2 +(3/16)cn^2 +(3/16)^2 cn^2 + ... + (3/16)^(log_4 n - 1) cn*2 =
  0(n^(log_4 3))


        = sum_ i = 0 ^ log_4 n - 1   (3/16)^i  cn^2 + 0(n^(log_4 3))  [1]

        = (3/16)^(log_4 n) - 1
          ---------------------  cn^2 + 0(n^(log_4 3))
          (3/16) - 1


  This last formula looks somewhat messy until we realize that we can again take
  advantage of small amounts of sloppiness and use an infinite decreasing
  geometric series as an upper bound.

    T(n) = [1]

         < sum ^ inf _i=0 ((3/16)^i) cn^2 + 0(n^(log_4 3))

         =    1
           ---------  cn^2 + 0(n^(log_4 3))
           1 - (3/16)

         = (16/13) cn^2 + 0(n^(log_4 3))

         = O(n^2)

  In fact, if O(n^2) is indeed an upper bound for the recurrence, then it must
  be a tight bound. Why? THe first recursive call contributes a cost of 0(n^2),
  and so Ω(n^2) must be a lower bound for the recurrence.

  Now we can use the substitution method to verify that our guess was correct.


    T(n) <= 3T(|_n/4_|) + cn^2
         <= 3d\_n/4_|^2 + cn^2
         <= 3d(n/4)^2 + cn^2
         <= (3/16)dn^2 + cn^2
         <= dn^2

    d >= (16/13)c


 e.g 2 T(n) = T(n/3) + T(2n/3) + O(n)



                        cn                                      cn

.           c(n/3)              c(2n/3)                         cn
.
.
.       c(n/9)  c(2n/9)     c(2n/9)     c(4n/9)                 cn
.
log_(3/2) n
.                       .
.                       .
.                       .
                                                                O(n lg n)


  Use substitution method to prove it.




- The master method for solving recurrences

  The master method provides a “cookbook” method for solving recurrences of the
  form:

            T(n) = aT(n/b) + f(n)

  where a >= 1 and b > 1 are constants and f(n) is an asymptotically positive
  function. To use the master method, you will need to memorize three cases, but
  then you will be able to solve many recurrences quite easily, often without
  pencil and paper.


  The recurrence describes the running time of an algorithm that divides a
  problem of size n into 'a' subproblems, each of size n/b, where a and b are
  positive constants. The 'a' subproblems are solved recursively, each in time
  T(n/b). The function f(n) encompasses the cost of dividing the problem and
  combining the results of subproblems.

  For example, the recurrence arising from Strassen's algorithm has z = 7, b =
  2, and f(n) = 0(n^2)


  As a matter of technical correctness, the recurrence is not actually well
  defined, because n/b might not be an integer.


  Replacing each of the 'a' terms T(n/b) with either T(|_n/b_|) or T(|~n/b~|)
  will not affect the asymptotic behavior of the recurrence, however. We
  normally find it convenient, therefore, to omit the floor and ceiling
  functions when writing divide-and-conquer recurrences of this form.


- The master theorem.
  The master method depends on the following theorem.


  Theorem 4.1 [pg 94]

  Let a >= 1 and b > 1 be constants, let f(n) be a function, and let T(n) has
  the following asymptotic bounds:

  - if f(n) = O(n^(log_b a - e)) for some constant e > 0, then 
    T(n) = 0(n^(log_b a))

  - if f(n) = 0(n^(log_b a)), then T(n) = 0(n^(log_b a) lg n)

  - if f(n) = Ω(n^(log_b a + e)) for some constant e > 0, and if

    a f(n/b) <= c f(n) for some constant c < 1 and all sufficiently large n,
    then 

        T(n) = 0(f(n))


    e.g T(n) = 9T(n/3) + n

      a = 9, b = 3, f(n) = n

          f(n) = O(n^(log_3 9 - e)), where e = 1, apply case 1

          T(n) = 0(n^2)

          more examples on page 95


 [ Proof of master theorem: Pg 97 ]



 [Strassen's method not as efficient as Coppersmith and Winograd]





* Probabilistic Analysis and Randomized Algorithms

[pg 1183 -- counting and probability Appendix]

  This appendix reviews elementary combinatorics and probability theory.


- Counting

  Counting theory tries to answer the question “How many?” without actually enu-
  merating all the choices. For example, we might ask, “How many different n-bit
  numbers are there?” or “How many orderings of n distinct elements are there?”
  In this section, we review the elements of counting theory. Since some of the
  material assumes a basic understanding of sets, you might wish to start by
  reviewing the material in Section B.1.

- Rules of sum and product

  We can sometimes express a set of items that we wish to count as a union of
  disjoint sets or as a Cartesian product of sets.


  [ End of appendix ]

- The hiring problem

  Suppose that you need to hire a new office assistant. Your previous attempts
  at hiring have been unsuccessful, and you decide to use an employment agency.
  The employment agency sends you one candidate each day. You interview that
  person and then decide either to hire that person or not. You must pay the
  employment agency a small fee to interview an applicant. To actually hire an
  applicant is more costly, however, since you must fire your current office
  assistant and pay a substantial hiring fee to the employment agency. You are
  committed to having, at all times, the best possible person for the job.
  Therefore, you decide that, after interviewing each applicant, if that
  applicant is better qualified than the current office assistant, you will fire
  the current office assistant and hire the new applicant. You are willing to
  pay the resulting price of this strategy, but you wish to estimate what that
  price will be.

  The procedure HIRE-ASSISTANT, given below, expresses this strategy for
  hiring in pseudocode. It assumes that the candidates for the office assistant
  job are numbered 1 through n.

  The procedure assumes that you are able to, after interviewing candidate i,
  determine whether candidate i is the best candidate you have seen so far. To
  initialize, the procedure creates a dummy candidate, numbered 0, who is less
  qualified than each of the other candidates.


    HIRE-ASSISTANT(n)
    1   best = 0
    2   for i = 1 to n
    3       interview candidate i
    4       if candidate i is better than candidate best
    5           best = i
    6           hire candidate i

  The cost model for this problem differs from the model described in Chapter 2.
  We focus not on the running time of HIRE-ASSISTANT, but instead on the costs
  incurred by interviewing and hiring. On the surface, analyzing the cost of
  this algorithm may seem very different from analyzing the running time of,
  say, merge sort.  The analytical techniques used, however, are identical
  whether we are analyzing cost or running time. In either case, we are counting
  the number of times certain basic operations are executed.

  Interviewing has a low cost, say ci, whreas hiring is expensive, costing ch.

  Letting m be the number of people hired, the total cost associated with this
  algorithm is O(ci n + chm). 

  No matter how many people we hire, we always interview n candidates and thus
  always incur the cost ci n associated with interviewing. We therefore
  concentrate on analyzing ch m, the hiring cost. This quantity varies with each
  run of the algorithm.

  This scenario serves as a model for a common computational paradigm. We often
  need to find the maximum or minimum value in a sequence by examining each
  element of the sequence and maintaining a current “winner.” The hiring problem
  models how often we update our notion of which element is currently winning.


- Worst-case analysis

  In the worst case, we actually hire every candidate that we interview. This
  situation occurs if the candidates come in strictly increasing order of
  quality, in which case we hire n times, for a total hiring cost of O(ch n).

  Of course, the candidates do not always come in increasing order of quality.
  In fact, we have no idea about the order in which they arrive, nor do we have
  any control over this order. Therefore, it is natural to ask what we expect to
  happen in a typical or average case.

- Probabilistic analysis

  Probabilistic analysis is the use of probability in the analysis of problems.
  Most commonly, we use probabilistic analysis to analyze the running time of an
  algorithm. Sometimes we use it to analyze other quantities, such as the hiring
  cost in procedure HIRE-ASSISTANT.

  In order to perform a probabilistic analysis, we must use knowledge of, or
  make assumptions about, the distribution of the inputs.  Then we analyze our
  algorithm, computing an average-case running time, where we take the average
  over the distribution of the possible inputs. Thus we are, in effect,
  averaging the running time over all possible inputs. When reporting such a
  running time, we will refer to it as the average-case running time.

  We must be very careful in deciding on the distribution of inputs. For some
  problems, we may reasonably assume something about the set of all possible in-
  puts, and then we can use probabilistic analysis as a technique for designing
  an efficient algorithm and as a means for gaining insight into a problem. For
  other problems, we cannot describe a reasonable input distribution, and in
  these cases we cannot use probabilistic analysis.

  For the hiring problem, we can assume that the applicants come in a random
  order. What does that mean for this problem? We assume that we can compare any
  two candidates and decide which one is better qualified; that is, there is a
  total order on the candidates.

  Thus, we can rank each candidate with a unique number from 1 through n, using
  rank(i) to denote the rank of applicant i, and adopt the convention that a
  higher rank corresponds to a better qualified applicant.

  The ordered list (rank(1), rand(2), ..., rank(n)) is a permutation of the list
  (1, 2,..., n). 


  Saying that the applicants come in a random order is equivalent to saying that
  this list of ranks is equally likely to be any one of the n! permutations of
  the numbers 1 through n.

  Alternatively, we say that the ranks form a uniform random permutation; that
  is, each of the possible n! permutations appears with equal probability.


- Randomized algorithms [pg 116]

  In order to use probabilistic analysis, we need to know something about the
  distribution of the inputs.

  In many cases, we know very little about the input distribution.  Even if we
  do know something about the distribution, we may not be able to model this
  knowledge computationally. Yet we often can use probability and randomness as
  a tool for algorithm design and analysis, by making the behavior of part of
  the algorithm random.

  In the hiring problem, it may seem as if the candidates are being presented to
  us in a random order, but we have no way of knowing whether or not they really
  are.  Thus, in order to develop a randomized algorithm for the hiring problem,
  we must have greater control over the order in which we interview the
  candidates. We will, therefore, change the model slightly. We say that the
  employment agency has n candidates, and they send us a list of the candidates
  in advance.

  On each day, we choose, randomly, which candidate to interview. Although we
  know nothing about the candidates (besides their names ), we have made a
  significant change.  Instead of relying on a guess that the candidates come to
  us in a random order, we have instead gained control of the process and
  enforced a random order.

  More generally, we call an algorithm randomized if its behavior is determined
  not only by its input but also by values produced by a random-number
  generator.

  We shall assume that we have at our disposal a random-number generator
  RANDOM. A call to RANDOM(a, b) returns an integer between a and b, inclusive,
  with each such integer being equally likely.

  For example, RANDOM(0, 1) produces 0 with probability 1/2, and it produces 1
  with probability 1/2.

  A call to RANDOM(3, 7) returns either 3, 4, 5, 6, or 7, each with probability
  1/5. Each integer returned by RANDOM is independent of the integers returned
  on previous calls.

  You may imagine RANDOM as rolling a (b - a + 1) - sided dice to obtain its
  output.

  When analyzing the running time of a randomized algorithm, we take the
  expectation of the running time over the distribution of values returned by
  the random number generator.

  We distinguish these algorithms from those in which the input is random by
  referring to the running time of a randomized algorithm as an expected running
  time. In general, we discuss the average-case running time when the
  probability distribution is over the inputs to the algorithm, and we discuss
  the expected running time when the algorithm itself makes random choices.


- Indicator random variables

  In order to analyze many algorithms, including the hiring problem, we use
  indicator random variables. Indicator random variables provide a convenient
  method for converting between probabilities and expectations. Suppose we are
  given a sample space S and an event A.

  Then the indicator random variable I{A} associated with event A is defined as:

   I{A} = 1     if A occurs,
          0     if A does not occur.


  As a simple example, let us determine the expected number of heads that we
  obtain when flipping a fair coin. Our sample space is S = {H, T}, with
  Pr{H} = Pr{T} = 1/2. We can then define an indicator random variable X_H,
  associated with the coin coming up heads, which is the event H.

  This variable counts the number of heads obtained in this flip, and it is 1 if
  the coin comes up heads and 0 otherwise.

  X_H = I{H}
      = 1       if H occurs
        0       if T occurs


  The expected number of heads obtained in one flip of the coin is simply the
  expected value of our indicator variable X_H:

    E[X_H] = E[I{H}]
           = 1 · Pr{H} + 0 · Pr{T}
           = 1 · (1/2) + 0 · (1/2)
           = 1/2

  Thus the expected number of heads obtained by one flip of a fair coin is 1/2.
  As the following lemma shows, the expected value of an indicator random
  variable associated with an event A is equal to the probability that A occurs.



- Lemma 5.1

  Given a sample space S and an event A in the sample space S, let X_A = I{A}.

  Then E{X_A} = PR{A}

  Proof: pg 119




  Let X be the random variable denoting the total number of heads in the n coin
  flips, so that

    X = Sum _i=1 ^n X_i


  We wish to compute the expected number of heads, and so we take the
  expectation of both sides of the above equation to obtain

    E[X] = E [ Sum_i=1 ^n Xi ]

        = Sum_i=1 ^n E[Xi]

        = Sum_i=1 ^n 1/2

        = n/2



- Analysis of the hiring problem using indicator random variables

  Returning to the hiring problem, we now wish to compute the expected number of
  times that we hire a new office assistant. In order to use a probabilistic
  anylysis, we assume that the candidates arrive in a random order, as discussed
  in the previous section.

  Let X be the random variable whose value equals the number of times we hire a
  new office assistant. We could then apply the definition of expected value to
  obtain:

    E[X] = Sum_x=1 ^n  x Pr{X = x}

  but this calculation would be cumbersome. We shall instead use indicator
  random variables to greatly simplify the calculation.

  To use indicator random variables, instead of computing E[X] by defining one
  variable associated with the number of times we hire a new office assistant,
  we define n variables related to whether or not each particular candidate is
  hired. In particular, we let Xi be the indicator random variable associated
  with the event in which the ith candidate is hired. Thus,

    Xi = I{candidate i is hired}

       = 1      if candidate i  is hired
         0      if candidate i  is not hired

  and

    X = X1 + X2 + ... + Xn

  By Lemma, we have that

    E[Xi] = Pr{candidate i is hired}

  and we must therefore compute the probability that lines 5-6 of HIRE-ASSISTANT
  are executed.

  Candidate i is hired, in line 6, exactly when candidate i is better than each
  of candidates 1 through i - 1. Because we have assumed that the candidates
  arrive in a random order, the first i candidates have appeared in a random
  order. Any one of these first i candidates is equally likely to be the
  best-qualified so far. Candidate i has a probability 1/i of being better
  qualified than candidates 1 through i - 1 and thus a probability of 1/i of
  being hired. By Lemma, we conclude that:

    E[Xi] = 1/i

  Now we can compute E[X]:

    E[X]  = E Sum_i=1 ^n Xi

          = Sum_i=1 ^n E[Xi]

          = Sum_i=1 ^n 1/i

          = ln n + O(1)

  Even though we interview n people, we actually hire only approximately ln n of
  them, on average. We summarize this result in the following lemma.

- Lemma 5.2

  Assuming that the candidates are presented in a random order, algorithm
  HIRE-ASSISTANT has an average-case total hiring cost of O(c_h ln n).

  Proof: pg 121

- Randomized algorithms

  In the previous section, we showed how knowing a distribution on the inputs
  can help us to analyze the average-case behavior of an algorithm. Many times,
  we do not have such knowledge, thus precluding an average-case analysis. As
  mentioned in Section 5.1, we may be able to use a randomized algorithm.

  For a problem such as the hiring problem, in which it is helpful to assume
  that all permutations of the input are equally likely, a probabilistic
  analysis can guide the development of a randomized algorithm. Instead of
  assuming a distribution of inputs, we impose a distribution. In particular,
  before running the algorithm, we randomly permute the candidates in order to
  enforce the property that every permutation is equally likely. Although we
  have modified the algorithm, we still expect to hire a new office assistant
  approximately ln n times.

  But now we expect this to be the case for any input, rather than for inputs
  drawn from a particular distribution.


  Let us further explore the distinction between probabilistic analysis and
  randomized algorithms.

   In Section 5.2, we claimed that, assuming that the candidates ar- rive in a
   random order, the expected number of times we hire a new office assistant is
   about ln n.

   Note that the algorithm here is deterministic; for any particular input, the
   number of times a new office assistant is hired is always the same.
   Furthermore, the number of times we hire a new office assistant differs for
   different inputs, and it depends on the ranks of the various candidates.
   Since this number depends only on the ranks of the candidates, we can
   represent a particular input by listing, in order, the ranks of the
   candidates, i.e (rank(1), rank(2), ..., rank(n)). Given the rank list

    A1 = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10), a new office assistant is always hired
    10 tomes, since each sucessive candidate is better than the previous one.


    Given the list of ranks A2 = (10, 9, 8, 7, 6, 5, 4, 3, 2, 1), a new office
    assistant is hired only once.


    A3 = (5, 2, 1, 8, 4, 7, 10, 9, 3, 6), a new office assistant is hired three
    times (5, 8, 10). 

    Expensive input: A1
    Inexpensive input: A2
    Moderately expensive: A3

    Consider, on the other hand, the randomized algorithm that first permutes
    the candidates and then determines the best candidate. In this case, we
    randomize in the algorithm, not in the input distribution.

    Given a particular input, say A3 above, we cannot say how many times the
    maximum is updated, because this quantity differs with each run of the
    algorithm.

    Each time we run the algorithm, the execution depends on the random choices
    made and is likely to differ from the previous execution of the algorithm.
    For this algorithm and many other randomized algorithms, no particular input
    elicits its worst-case behavior.

    Even your worst enemy cannot produce a bad input array, since the random
    permutation makes the input order irrelevant. The randomized algorithm
    performs badly only if the random-number generator produces an “unlucky”
    permutation.

    For the hiring problem, the only change needed in the code is to randomly
    permute the array.




        RANDOMIZED-HIRE-ASSISTANT(n)
        1   randomly permute the list of candidates
        2   best = 0

        3   for i = 1 to n
        4       interview candidate i
        5       if candidate i is better than candidate best
        6           best = i
        7           hire candidate i

    With this simple change, we have created a randomized algorithm whose
    performance matches that obtained by assuming that the candidates were
    presented in a random order.

################################################################################
- Lemma 5.2

  Assuming that the candidates are presented in a random order, algorithm
  HIRE-ASSISTANT has an average-case total hiring cost of O(c_h ln n).
################################################################################

- Lemma 5.3

  The expected hiring cost of the procedure RANDOMIZED-HIRE-ASSISTANT is
  O(Ch ln n)



  Comparing Lemmas 5.2 and 5.3 highlights the difference between probabilistic
  analysis and randomized algorithms.

  In Lemma 5.2, we make an assumption about the input. In Lemma 5.3, we make no
  such assumption, although randomizing the input takes some additional time.


  To remain consistent with our terminology, we couched Lemma 5.2 in terms of
  the average-case hiring cost and Lemma 5.3 in terms of the expected hiring
  cost.





- Randomly permuting arrays

  Many randomized algorithms randomize the input by permuting the given input
  array. (There are other ways to use randomization. ) Here, we shall discuss
  two methods for doing so. We assume that we are given an array A which,
  without loss of generality, contains the elements 1 through n. Our goal is to
  produce a random permutation of the array.

  One common method is to assign each element A[i] of the array a random
  priority P[i], and ten sort the elements of A according to these priorities.
  For example, in our initial array is A = (1, 2, 3, 4) and we choose random
  priorities P = (36,3, 62, 19), we would produce an array B = (2, 4, 1, 3),.


  We call this procedure PERMUTE-BY-SORTING:

    PERMUTE-BY-SORTING(A)

    1   n = A.length
    2   let P[1..n] be a new array
    3   for i = 1 to n
    4       P[i] = RANDOM(1, n^3)
    5   sort A, using P as sort keys


  Line 4 chooses a random number between 1 and n^3. We use a range of 1 to n^3
  to make it likely that all the priorities in P are unique.

  The time-consuming step in this procedure is the sorting in line 5.

  Sorting takes  Ω(n lg n) time. After sorting, if P[i] is the j th smallest
  priority, then A[i] lies in position j of the output. In this manner, we
  obtain a permutation.

  It remains to prove that the procedure produces a uniform random permutation,
  that is, that the procedure is equally likely to produce every permutation of
  the numbers 1 through n.


- Lemma 5.4

  Procedure PERMUTE-BY-SORTING produces a uniform random permutation of the
  input, assuming that ll priorities are distinct.

  Proof: pg 125




  A better method for generating a random permutation is to permute the given
  array in place. The procedure RANDOMIZE-IN-PLACE does so in O(n) time. In its
  ith iteration, it chooses the element A[i] randomly from among elements A[i]
  through A[n]. Subsequent to the ith iteration, A[i] is never altered.


    RANDOMIZE-IN-PLACE(A)
    1   n = A.length
    2   for i = i to n
    3   swap A[i] with A[RANDOM(i, n)]

  We shall use a loop invariant to show that procedure R ANDOMIZE -I N -P LACE
  produces a uniform random permutation. A k-permutation on a set of n elements
  is a sequence containing k of the n elements, with no repetitions. There are

        n!
    ---------  such possible k-permutations
    (n - k) !


- Lemma 5.5

  Procedure RANDOMIZE-IN-PLACE computes a uniform random permutation.

  Proof: pg 127


- Probabilistic analysis and further uses of indicator random variables

  This advanced section further illustrates probabilistic analysis by way of
  four examples. The first determines the probability that in a room of k
  people, two of them share the same birthday. The second example examines what
  happens when we randomly toss balls into bins. The third investigates
  “streaks” of consecutive heads when we flip coins. The final example analyzes
  a variant of the hiring problem in which you have to make decisions without
  actually interviewing all the candidates.


- The birthday paradox

  Our first example is the birthday paradox. How many people must there be in a
  room before there is a 50% chance that two of them were born on the same day
  of the year? The answer is surprisingly few. The paradox is that it is in fact
  far fewer than the number of days in a year, or even half the number of days
  in a year, as we shall see.

  To answer this question, we index the people in the room with the integers
  1,2,...,k, where k is the number of people in the room. We ignore the issue of
  leap years and assume that all years have n = 365 days.

  For i = 1, 2, ..., k, let bi be the day of the year on which person i's
  birthday falls, where 1 <= bi <= n.

  We also assume that birthdays are uniformly distributed across the n days of
  the year, so that

    Pr {bi = r} = 1/n for i = 1, 2, ..., k and r = 1, 2, ..., n


  The probability that two given people, say i and j, have matching birthdays
  depends on whether the random seleciton of birthdays is independent. We assume
  from now on that birthdays are independent, so that the probability that i's
  birthday and j's birthday both fall on day r is:

    Pr {bi = r and bj =r} = Pr{bi = r} Pr {bj = r}
                          = 1/n^2

  Thus, the probability that they both fall on the same day is:

    Pr{bi = bj} = sum_r=1 ^n Pr{bi=r and bj =r}

                = sum_r=1 ^n (1/n^2)

                = 1/n

  More intuitively, once bi is chosen, the probability that bj is chosen to be
  the same day is 1/n. Thus, the probability that i and j have the same birthday
  is the same as the probability that the birthday of one of them falls on a
  given birthday.

  Notice, however, that this coincidence depends on the assumption that the
  birthdays are independent.


  We can analyze the probability of at least 2 out of k people having matching
  birthdays by looking at the complementary event. The probability that at least
  two of the birthdays match is 1 minus the probability that all the birthdays
  are different.  The event that k people have distinct birthdays is

           k
    Bk =   n Ai
          i=1

    where Ai is the event that person i's birthday is different from person j's
    for all j < i.

    [continued pg 131]



* Sorting and Order Statistics

- Introduction

  This part presents several algorithms that solve the following sorting
  problem:

  Input:  A sequence of n numbers (a1, a2, ..., an)

  Output:A permtation (reordering) (a1', a2', ..., an') of the input sequence
  such that a1' <= a2' <= ... <= an'

  The input sequence is usually an n-element array, although it may be
  represented in some other fashion ,such as a linked list.

- The structure of the data

  In practice, the numbers to be sorted are rarely isolated values. Each is
  usually part of a collection of data called a record. Each record contains a
  key, which is the value to be sorted. The remainder of the record consists of
  satellite data, which are usually carried around with the key. In practice,
  when a sorting algorithm permutes the keys, it must permute the satellite data
  as well. If each record includes a large amount of satellite data, we often
  permute an array of pointers to the records rather than the records themselves
  in order to minimize data movement.

  In a sense, it is these implementation details that distinguish an algorithm
  from a full-blown program. A sorting algorithm describes the method by which
  we determine the sorted order, regardless of whether we are sorting individual
  numbers or large records containing many bytes of satellite data. Thus, when
  focusing on the problem of sorting, we typically assume that the input
  consists only of numbers. Translating an algorithm for sorting numbers into a
  program for sorting records is conceptually straightforwards, although i na
  given engineering situation other subtleties may make the actual programming
  task a challenge.


- Sorting algorithms

  We introduced two algorithms that sort n real numbers in Chapter 2. Insertion
  sort takes 0(n^2) time in the worst case. Bcause its inner loops are tight,
  however, it is a fast in-place sorting algorithm for small input sizes.

  (Recall that a sorting algorithm sorts in place if only a constant number of
  elements of the input array are ever stored outside the array.) 

  Merge sort has a better asymptotic running time, 0(n lg n), but the MERGE
  procedure it uses does not operate in place.


  In this part, we shall introduce two more algorithms that sort arbitrary real
  numbers. 

  Heapsort, presented in Chapter6, sorts n numbers in place in O(n lg n) time.
  It uses an important data structure, called a heap, with which we can also
  implement a priority queue.

  Quicksort, in Chapter 7, also sorts n numbers in place, but its worst-case
  running time is 0(^2). Its expected running time is 0(n lg n), however, and
  it generally outperforms heapsort in practice. Like insertion sort, quicksort
  has tight code, and so the hidden constant factor in its running time is
  small. It is a popular algorithm for sorting large input arrays.

  Insertion sort, merge sort, heapsort, and quicksort are all comparison sorts:
  they determine the sorted order of an input array by comparing elements.

  Chapter 8 begins by introducing the decision-tree model in order to study the
  performance limitations of comparison sorts. using this model, we prove a
  lower bound of Ω(n lg n) on the worst-case running time of any comparison sort
  of n inputs, thus showing that heapsort and merge sort are asymptotically
  optimal comparison sorts.


  Chapter 8 then goes on to show that we can beat this lower bound of Ω(n lg n)
  if we can gather information about the sorted order of the input by means
  other than comparing elements.

  The counting sort algorithm, for example, assumes that the input numbers are
  in the set {0, 1, ..., k}. By using array indexing as a tool for determining
  relative order, counting sort can sort n numbers in 0(k +n) time.

  Thus, when k = O(n), counting sort runs in time that is linear in the size of
  the input array. A related algorithm, radix sort, can be used to extend the
  range of counting sort. If there are n integers to sort, each integer has d
  digits, and each digit can take on up to k possible values, then radix sort
  can sort the numbers in 0(d(n +k)) time.

  When d is a constant and k is O(n), radix sort runs in linear time. A third
  algorithm, bucket sort, requires knowledge of the probabilistic distribution
  of number in the input array. It can sort n real numbers uniformly distributed
  in the half-open interval [0, 1) in average-case O(n) time.

  [Algorithm table in pg 150 -- show sort running times]


- Order statistics

  The ith order statistic of a set of n numbers is the ith smallest number in
  the set. We can, of course, select the ith order statistic by sorting hte
  input and indexing the ith element of the output. With no assumptions about
  the input distribution, the method runs in  Ω(n lg n) time, as the lower bound
  proved in Chapter 8 shows.

  In chapter 9, we show that we can find the ith smallest element in O(n) time,
  even when the elements are arbitrary real numbers. We present a randomized
  algorithm with tight pseudocode that runs in 0(n^2) time in the worst case,
  but whose expected running time is O(n). We also give a more complicated
  algorithm that runs in O(n) worst-case time.

- Background

  Although most of this part does not rely on difficult mathematics, some
  sections do require mathematical sophistication. In particular, analyses of
  quicksort, bucket sort, and the order-statistic algorithm use probability,
  which is reviewed in Appendix C, and the material on probabilistic analysis
  and randomized algorithms in Chapter 5. The analysis of the worst-case
  linear-time algorithm for order statistics involves somewhat more
  sophisticated mathematics than the other worst-case analyses in this part.



* Heapsort [pg 151]

  Like merge sort, but unlike insertion sort, heapsort’s running time is O(n lg
  n). Like insertion sort, but unlike merge sort, heapsort sorts in place: only
  a constant number of array elements are stored outside the input array at any
  time. Thus, heapsort combines the better attributes of the two sorting
  algorithms we have already discussed.

  Heapsort also introduces another algorithm design technique: using a data
  structure, in this case one we call a “heap,” to manage information. Not only
  is the heap data structure useful for heapsort, but it also makes an efficient
  priority queue. The heap data structure will reappear in algorithms in later
  chapters.

  The term “heap” was originally coined in the context of heapsort, but it has
  since come to refer to “garbage-collected storage,” such as the programming
  languages Java and Lisp provide. Our heap data structure is not
  garbage-collected storage, and whenever we refer to heaps in this book, we
  shall mean a data structure rather than an aspect of garbage collection.


- Heaps

  The (binary) heap data structure is an array object that we can view as a
  nearly complete binary tree. Each node of the tree corresponds to an element
  of the array. The tree is completely filled on all levels except possibly the
  lowest, which is filled from the left up to a point. An array A that
  represents a heap is an object with two attributes: A.length, which (as usual)
  gives the number of elemnts in the array, and A.heap-size, which represents
  how many elements in the heap are stored within array A.

  That is, although A[1..A.length] may contain numbers, only the elements in
  A[1..A.heap-size], where 0 <= A.heap-size <= A.length, are valide elements of
  the heap. The root of the tree is A[1], and given the index i of a node, we
  can easily compute the indices of its parent, left child, and right child.



                                    1  2  3  4 5 6 7
           1                        16 14 10 8 7 9 3
           16

    2               3
   14               10

4       5      6           7
8       7      9           3


  The tree has height 3; the node of index 4 (with value 8) has height one.


    PARENT(i)
    1   return |_i/2_|

    LEFT(i)
    1   return 2i

    RIGHT(i)
    1   return 2i + 1

  On most computers, the LEFT procedure can compute 2i in one instruction by
  simply shifting the binary representation of i left by one bit position.
  Similarly, the RIGHT procedure can quickly compute 2i + 1 by shifting the
  binary representation of i left by one bit position and then adding in a 1 as
  the low-order bit. The PARENT procedure can compute |_i/2_| by shifting i
  right one bit position. Good implementations of heapsort often implement these
  procedures as "macros" or "inline" procedures.

  There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds,
  the values in the nodes satisfy a heap property, the specifics of which depend
  on the kind of heap. In a max-heap, the max-heap property is that for every
  node i other than the root,

    A[PARENT(i)] >= A[i]

  that is, the value of a node is at most the value of its parent. Thus, the
  largest element in a max-heap is stored at the root, and the subtree rooted at
  a node contains values no larger that that contained at the node itself. A
  min-heap is organized in the opposite way; the min-heap property is that for
  every node i other than the root,

    A[PARENT(i)] <= A[i]

  The smallest element in a min-heap is at the root.


  For the heapsort algorithm, we use max-heaps. Min-heaps commonly implement
  priority queues, which we discuss in Section 6.5. We shall be precise in
  specifying whether we need a max-heap or a min-heap for any particular
  application, and when properties apply to either max-hearps or min-heaps, we
  just use the term "heap."

  Viewing a heap as a tree, we define the height of a node in a heap to be the
  number of edges on the longest simple downward path from the node of a leaf,
  and we define the height of the heap to be the height of its root. Since a
  heap of n elements is based on a complete binary tree, its height is 0(lg n)

  We shall see that the basic operatons on heaps run in time at most
  proportional to the height of the tree and thus take O(lg n) time. The
  remainder of this chapter presents some basic procedures and shows how they
  are used in a sorting algorithm and a priority-queue data structure.

  - The MAX-HEAPIFY procedure, which runs in O(lg n) time, is the key to
    maintaining the max-heap property.

  - The BUILD-MAX-HEAP procedure, which runs in linear time, produces a max-heap
    from an unordered input array.

  - The HEAPSORT procedure, which runs in O(n lg n) time, sorts an array in
    place.

  - The MAX-HEAP-INSERT, HEAP-EXTRACT-MAX, HEAP-INCREASE-KEY, and HEAP-MAXIMUM
    procedures, which run in O(ln n) time, allow the heap data structure to
    implement a priority queue.


- Maintaining the heap property

  In order to maintain the max-heap property, we call the procedure MAX-HEAPIFY.
  Its inputs are an array A and an index i into the array. When it is called,
  MAX-HEAPIFY assumes that the binary trees rooted at LEFT(i) and RIGHT(i) are
  max-heaps, but that A[i] might be smaller than its children, thus violating
  the max-heap property. MAX-HEAPIFY lets the value at A[i] "float-down" in the
  max-heap so that the subtree rooted at index i obeys the max-heap property.


    MAX-HEAPIFY(A, i)
    1   l = LEFT(i)
    2   r = RIGHT(i)
    3   if l <= A.heap-size and A[l] > A[i]
    4       largest = l
    5   else largest = i
    6   if r <= A.heap-size and A[r] > A[largest]
    7       largest = r
    8   if largest != i
    9       exchange A[i] with A[largest]
    10      MAX-HEAPIFY(A, largest)

    [Illustration on page 155]

  The running time of MAX-HEAPIFY on a subtree of size n rooted at a given node
  i is the 0(1) time to fix up the relationships among the elements A[i],
  A[Left(i)], and A[RIGHT(i)], plus the time to run MAX-HEAPIFY on a subtree
  rooted at one of the children of node i (assuming that the recursive call
  occurs). The children's subtrees each have size at most 2n/3 - the worst case
  occurs when the bottom level of the tree is exactly half full - and therefore
  we can describe the running time of MAX-HEAPIFY by the recurrence:

    T(n) <= T(2n/3) + 0(1)

  The solution to this recurrence, by case 2 of the master theorem, is 

    T(n) = O(lg n)


- Building a heap

  We can use the procedure MAX-HEAPIFY in a bottom-up manner to convert an array
  A[1..n], where n = A.length, into a max-heap.

  The procedure BUILD-MAX-HEAP goes through the remaining nodes of the tree and
  runs MAX-HEAPIFY on each one.

    BUILD-MAX-HEAP(A)
    1   A.heap-size = A.length
    2   for i = |_A.length/2_| downto 1
    3       MAX-HEAPIFY(A, i)

  We need to show that this invariant is true prior to the first loop iteration,
  that each iteration of the loop maintains the invariant, and that the
  invariant provides a useful property to show correctness when the loop
  terminates.


  Initialization: Prior to the first iteration of the loop, i = |_n/2_|. Each
  node |_n/2_| + 1, |n/2_| +2, ..., n is a leaf and is thus the root of a
  trivial max-heap.

  Maintainance: To see that each iteration maintains the loop invariant, observe
  that the children of node i are numbered higher than i. By the loop invariant,
  therefore, they are both roots of max-heaps.

  This is precisely the condition required for the call MAX-HEAPIFY(A, i) to
  make the node i a max-heap root. Moreover, the MAX-HEAPIFY call preserves the
  property that nodes i + 1, i + 2, ..., n are all roots of max-heaps.
  Decrementing i in the for loop update reestablishes the loop invariant for the
  next iteration.

  Termination: At termination, i = 0. By the loop invariant, each node 1, 2,
  ..., n is the root of a max-heap. In particular, node 1 is.



  We can compute a simpler upper bound on the running time of BUILD-MAX-HEAP as
  follows. Each call to MAX-HEAPIFY costs O(lg n) time, and BUILD-MAX-HEAP makes
  O(n) such calls. Thus the running time is O(n lg n). This upper bound, though
  correct, is not asymptotically tight.

  [tighter analysis: pg 157]

  [tight bound result: O(n)]

  Hence, we can build a max-heap from an unordered array in linear time.

  We can build a min-heap by the procedure BUILD-MIN-HEAP, which is the
  same as BUILD-MAX-HEAP but with the call to MAX-HEAPIFY in line 3
  replaced by a call to MIN-HEAPIFY

  BUILD-MIN-HEAP produces a min-heap from an unordered linear array in linear
  time.



- The heapsort algorithm

  The heapsort algorithm starts by using BUILD-MAX-HEAP to build a max-heap on
  the input array A[1..n], where n = A.length. Since the maximum element of the
  array is stored at the root A[1], we can put it into its correct final
  position by exchanging it with A[n]. If we now discard node n from the heap -
  and we can do so by simply decrementing A.heap-size - we observe that the
  children of the root remain max-heaps, but the new root element might violate
  the max-heap property. All we need to do to restore the max-heap property,
  however, is call MAX-HEAPIFY(A, 1), which leaves a max-heap in A[1..n - 1].
  The heapsort algorithm then repeats this process for the max-heap of size
  n - 1 does to a heap of size 2.

    HEAPSORT(A)
    1   BUILD-MAX-HEAP(A)
    2   for i = A.length downto 2
    3       exchange A[1] with A[i]
    4       A.heap-size = A.heap-size - 1
    5       MAX-HEAPIFY(A, 1)

    [ it's about using a max-heap to sort numbers ]
    [illustration pg 161]
  The HEAPSORT procedure takes time O(n lg n), since the call to BUILD-MAX-HEAP
  takes time O(n) and each of the n - 1 calls to MAX-HEAPIFY takes time O(lg n).


- Priority Queues

  Heapsort is an excellent algorithm, but a good implementation of quicksort,
  presented in Chapter 7, usually beats it in practice. Nevertheless, the heap
  data structure itself has many uses. In this section, we present one of the
  most popular applications of a heap: as an efficient priority queue. As with
  heaps, priority queues come in two forms: max-priority queues and min-priority
  queues.

  We will focus here on how to implement max-priority queues, which are in turn
  based on max- heaps.



  A priority queue is a data structure for maintaining a set S of elements, each
  with an associated value called a key. A max-priority queue supports the
  following operations:

    INSERT(S, x) inserts the element x into the set S, which is equivalent to
    the operation S = S U {x}

    MAXIMUM(S) returns the element of S with the largest key.

    EXTRACT-MAX(S) removes and returns the element of S with the largest key.

    INCREASE-KEY(S, x, k) increases the value of element x's key to the new
    value k, which is assumed to be at least as large as x's current key value.



  Among their other applications, we can use max-priority queues to schedule
  jobs on a shared computer. The max-priority queue keeps track of the jobs to
  be performed and their relative priorities. When a job is finished or
  interrupted, the scheduler selects the highest-priority job from among those
  pending by calling EXTRACT-MAX. The scheduler can add a new job to the queue
  at any time by calling INSERT.

  Alternatively, a min-priority queue supports the operations INSERT, MINIMUM,
  EXTRACT-MIN, and DECREASE-KEY. A min-priority queue can be used in an
  event-driven simulator. The items in the queue are events to be simulated,
  each with an associated time of occurrence that serves as its key. The events
  must be simulated in order of their time of occurrence, because the simulation
  of an event can cause other events to be simulated in the future.  The
  simulation program calls EXTRACT-MIN at each step to choose the next event to
  simulate. As new events are produced, the simulator inserts them into the
  min-priority queue by calling INSERT.


  Not surprisingly, we can use a heap to implement a priority queue. In a given
  application, such as job scheduling or event-driven simulation, elements of a
  priority queue correspond to objects in the application. We often need to
  determine which application object corresponds to a given priority queue
  element, and vice versa.  When we use a heap to implement a priority queue,
  therefore, we often need to store a handle to the corresponding application
  object in each heap element.  The exact makeup of the handle (such as a
  pointer or an integer ) depends on the application.  Similarly, we need to
  store a handle to the corresponding heap element in each application object.
  Here, the handle would typically be an array index.

  HEAP-MAXIMUM operation takes 0(1) time.


    HEAP-MAXIMUM(A)
    1   return A[1]


  The procedure HEAP-EXTRACT-MAX implements the EXTRACT-MAX operation. It is
  similar to the for loop body (lines 3–5 ) of the H EAPSORT procedure.

    HEAP-EXTRACT-MAX(A)
    1   if A.heap-size < 1
    2       error "heap underflow"
    3   max = A[1]
    4   A[1] = A[A.heap-size]
    5   A.heap-size = A.heap-size - 1
    6   MAX-HEAPIFY(A, 1)
    7   return max


  Running time of HEAP-EXTRACT-MAX is O(lg n).



  The procedure HEAP-INCREASE-KEY implements the INCREASE-KEY operation. An
  index i into the array identifies the priority-queue element whose key we wish
  to increase. The procedure first updates the key of element A[i] to its new
  value. Because increasing the key of A[i] might violate the max-heap property,
  the procedure then, in a manner reminiscent of the insertion loop (lines 5 -
  7) of INSERSION-SORT, traverses a simple path from this node toward the root
  to find a proper place for the newly increased key. 

  As HEAP-INCREASE-KEY traverses this path, it repeatedly compares an element to
  its parent, exchanging their keys and continuing if the element's key is
  larger, and terminating if the lement's key is smaller, since the max-heap
  property now holds.

    HEAP-INCREASE-KEY(A, i, key)
    1   if key < A[i]
    2       error "new key is smaller than current key"
    3   A[i] = key
    4   while i > 1 and A[PARENT(i)] < A[i]
    5       exchange A[i] with A[PARENT(i)]
    6       i = PARENT(i)

    [pg 165]


  The procedure MAX-HEAP-INSERT implements the INSERT operation. It takes as an
  input the key of the new element to be inserted into max-heap A. The procedure
  first expands the max-heap by adding to the tree a new leaf whose key is -
  inf.

  Then it calls HEAP-INCREASE-kEY to set the key of this new node to its correct
  value and maintain the max-heap property.

    MAX-HEAP-INSERT(A, key)
    1   A.heap-size = A.heap-size + 1
    2   A[A.heap-size] = - inf
    3   HEAP-INCREASE-KEY(A, A.heap-size, key)



- Quicksort

  Quicksort worst-case running time: 0(n^2)

  Efficient on the average: expected running time: 0(n lg n)
  and the constant factors hidden in the 0(n lg n) notation are quite small. It
  also has the advantage of sorting in place, and it works well even in
  virtual-memory environments.

- Description of quicksort

  Applies the divide-and-conquer paradigm.


  Divide: Partition the array A[p..r] into two subarrays A[p..q - 1] and 
  A[1 + 1.. r] such that each element of A[p..q - 1] is less than or equal to
  A[q], which is, in turn, less than or equal to each element of A[q + 1..r].
  Compute the index q as part of this partitioning procedure.

  Conquer: Sort the two subarrays A[p..q-1] and A[q + 1..r] by recusive calls to
  quicksort

  Combine: Because the subarrays are already sorted, no work is needed to
  combine them.


    QUICKSORT( A, p, r )
    1   if p < r
    2       q = PARITION(A, p, r)
    3       QUICKSORT(A, p, q - 1)
    4       QUICKSORT(A, q + 1, r)

  To sort an entire array, A, the initial call is QUICKSORT(A, 1, A.length)


  The key to the algorithm is the PARTITION procedure, which rearranges the
  subarray A[p..r] in place.

    PARTITION(A, p, r)
    1   x = A[r]
    2   i = p - 1
    3   for j = p to r - 1
    4       if A[j] <= x
    5           i = i + 1
    6           exchange A[i] with A[j]
    7   exchange A[i + 1] with A[r]
    8   return i + 1

    [pg 172 for illustration]

  PARITION always selects an element x = A[r] as a pivot element around which to
  parition the subarray A[p..r]. As the procedure runs, it partitions the array
  into four (possibly empty) regions. At the start of each iteration of the for
  loop in lines 3-6, the regions satisfy certain properties, shown in Fig 7.2
  [pg 173]. We state these properties as a loop invariant:

  At the beginning of each iteration of the loop of lines 3-6, for any array
  index k,

  1. If p <= k <= i, then A[k] <= x
  2. If i + 1 <= k <= j - 1, then A[k] > x
  3. If k = r, then A[k] = x


  Initialization: Prior to the first iteration, i = p - 1 and j = p. Because no
  values lie between p and i and no values lie between i + 1 and j -1, the first
  two condition are satisfied. THe assignment in line 1 satisfies the third
  condition.


  Maintenance: We consider 2 cases, depending on the outcome of hte test in line
  4. [See fig 7.3 pg 174]

  Termination: At terminatio, j = r. Therefore, every entry in the array is in
  one of the three sets described by the invariant, and we have partitioned the
  values in the array into three sets: those less than or equal to x, those
  greater than x, and a singleton set containing x.

  Running time is 0(n).

- Performace of Quicksort

  The running time of quicksort depends on whether the partitioning is balanced
  or unbalanced, which in turn depends on which elements are used for
  partitioning.  If the partitioning is balanced, the algorithm runs
  asymptotically as fast as merge sort. If the partitioning is unbalanced,
  however, it can run asymptotically as slowly as insertion sort.

  [analysis of pg 175]


- A randomized version of quicksort

  In exploring the average-case behavior of quicksort, we have made an
  assumption that all permutations of the input numbers are equally likely. In
  an engineering situation, however, we cannot always expect this assumption to
  hold.

    RANDOMIZED-PARTITION(A, p, r)
    1   i = RANDOM(p, r)
    2   exchange A[r] with A[i]
    3   return PARITION(A, p, r)

    RANDOMIZED-QUICKSORT(A, p, r)
    1   if p < r
    2       q = RANDOMIZED-PARTITION(A, p, r)
    3       RANDOMIZED-QUICKSORT(A, p, q - 1)
    4       RANDOMIZED-QUICKSORT(A, q + 1, r)



 [more on analysis in the following sections]

- Sorting in Linear time [pg 191]

  The algorithms discussed above share an interesting property: the sorted order
  they determine is based only on comparisons between the input elements. We
  call such sorting algorithms comparison sorts.


  We can prove that any comparison sort must take Ω(n lg n) comparisons in the
  worst case to sort n elements.

  [section 8.1 pg 191 shows the proof]


- Counting sort

  Counting sort assumes that each of the n input elements is an integer in the
  range of 0 to k, for some integer k. When k = O(n), the sort runs in 0(n)
  time.

  Counting sort determines, for each input element x, the number of elements
  less than x. It uses this information to place element x directly into its
  position in the output array. For example, if 17 elements are less than x,
  then x belongs in output position 18. We must modify this scheme slightly to
  handle the situation in which several elements have the same value, since we
  do not want to put them all in the same position.

  We assume:
  input is an array A[1..n] , A.length = n
  B[1..n] holds the sorted output
  C[0..k] provides temporary working storage

    COUNTING-SORT(A, B, k)
    1   let C[0..k] be a new array

    2   for i = 0 to k
    3       C[i] = 0

    4   for j = 1 to A.length
    5       C[A[j]] = C[A[j]] + 1

    6   // C[i] now contains the number of elements equal to i
    7   for i = 1 to k
    8       C[i] = C[i] + C[i - 1]

    9   // C[i] now contains the number of elements less than or equal to i
    10  for j = A.length downto 1
    11      B[C[A[j]]] = A[j]
    12      C[A[j]] = C[A[j]] - 1

  [pg 195 for illustration]

  An important property of counting sort is that it is stable: numbers with the
  same value appear in the output array in the same order as they do in the
  input array. That is, it breaks ties between two numbers by the rule that
  whichever number appears first in the input array appears first in the output
  array. Normally, the property of stability is important only when satellite
  data are carried around with the element being sorted. Counting sort’s
  stability is important for another reason: counting sort is often used as a
  subroutine in radix sort. As we shall see in the next section, in order for
  radix sort to work correctly, counting sort must be stable.

- Radix sort

    RADIX-SORT(A, d)
    1   for i = 1 to d
    2       use a stable sort to sort array A on digit i


    so 123 345 567

  -- personal explanation --
  it will starts with the least significant digits [3, 5, 7], sort the numbers
  based on the least significant digits, then move the to second-least
  significant digits, sort the numbers, etc, till you reach the most significant
  digit. [pt 198 for illustration]


  In order for radix sort to work correctly, the digit sorts must be stable.
  The sort performed by a card sorter is stable, but the operator has to be wary
  about not changing the order of the cards as they come out of a bin, even
  though all the cards in a bin have the same digit in the chosen column.

  In a typical computer, which is a sequential random-access machine, we some-
  times use radix sort to sort records of information that are keyed by multiple
  fields.  For example, we might wish to sort dates by three keys: year, month,
  and day. We could run a sorting algorithm with a comparison function that,
  given two dates, compares years, and if there is a tie, compares months, and
  if another tie occurs, compares days. Alternatively, we could sort the
  information three times with a stable sort: first on day, next on month, and
  finally on year.


- Bucket sort

  Bucket sort assumes that the input is drawn from a uniform distribution and
  has an average-case running time of O(n).

  Like counting sort, bucket sort is fast because it assumes something about the
  input. Whereas counting sort assumes that the input consists of integers in a
  small range, bucket sort assumes that the input is generated by a random
  process that distributes elements uniformly and independently over the
  interval [0, 1).

  Bucket sort divides the interval [0, 1) into n equal-sized subintervals, or
  buckets and then distributes the n input numbers into the buckets.

  pg 201 for illustration


    BUCKET-SORT(A)
    1   let B[0..n-1] be a new array
    2   n = A.length

    3   for i = 0 to n - 1
    4       make B[i] and empty list

    5   for i = 1 to n
    6       insert A[i] into list B[[nA[i]]]

    7   for i = 0 to n - 1
    8       sort list B[i] with insertion sort

    9   concatenate the lists B[0], B[1], ..., B[n-1] together in order.


    [read more about the stable thing]




* Medians and Order Statistics

  The i th order statistic of a set of n elements is the i th smallest element.
  For example, the minimum of a set of elements is the first order statistic
  (i = 1) and the maximum is the n th order statistic (i = n).

  A median, informally, is the "halfway point" of the set. When n is odd, the
  median is uniqu, occuring at i = (n + 1) / 2.

  When n is even, there are two medians, occurring at i = n / 2 and
  i = n / 2 + 1. Thus, regardless of the parity of n, medians occur at

  i = |_ (n + 1) / 2 _| and i = |~ ( n + 1 ) / 2 ~|. For simplicity, however, we
  refer to the lower median as the median.

  This chapter addresses the problem of selecting the i th order statistic from
  a set of n distinct numbers. We assume for convenience that the set contains
  distinct numbers, although virtually everything that we do extends to the
  situation in which a set contains repeated values. We formally specify the
  selection problem as follows:

    Input: A set A of n (distinct) numbers and an integer i, with 1 <= i <= n.

    Output: The element x E A that is larger than exactly i - 1 other elements
    of A.



  We can solve the selection problem in O(n lg n) time, since we can sort the
  numbers using heapsort or merge sort and then simply index the i the element
  in the output array. This chapter presents faster algorithms.


- Minimum and Maximum

  How many comparisons are necessary to determine the minimum of a set of n
  elements? We can easily obtain an upper bound of n - 1 comparisons: examine
  each element of the set in turn and keep track of the smallest element seen so
  far. In the following procedure, we assume that the set resides in array A,
  where A.length = n.

    MINIMUM(A)
    1   min = A[1]
    2   for i = 2 to A.length
    3       if min > A[i]
    4           min = A[i]
    5   return min

  We can. of course, find the maximum with n - 1 comparisons as well.

  Is this the best we can do? Yes, since we can obtain a lower bound of n - 1
  comparisons for the problem determining the minimum.


- Simultaneous minimum and maximum

  In some applications, we must find both the minimum and the maximum of a set
  of n elements. For example, a graphics program may need to scale a set of
  (x,y) data to fit onto a rectangular display screen or other graphical output
  device. To do so, the program must first determine the minimum and maximum
  value of each coordinate.

  At this point, it should be obvious how to determine both the minimum and the
  maximum of n elements using 0(n) comparisons. Using n - 1 comparisons for
  each, for a total of 2n - 2 comparisons.

  In fact, we can find both the minimum and the maximum using at most
  3 |_ n / 2 _| comparisons. We do so by maintaining both the minimum and
  maximum elements seen thus far.

  Rather than processing each element of the input by comparing it against the
  current minimum and maximum, at a cost of 2 comparisons per element, we
  process elements in pairs.

  We compare pairs of elements from the input first with each other, and then we
  compare the smaller with the current minimum, and the larger to the current
  maximum, at a cost of 3 comparisons for every 2 elements.

  How we set up initial values for the current minimum and maximum depends on
  whether n is odd or even.

  If n is odd, we set both the minimum and maximum to the value of the first
  element. If n is even, we perform 1 comparison on the first 2 elements to
  determine the initial values of the minimum and maximum, and then process the
  rest of the elements in pairs as in the case for odd n.

  If n is odd, we perform 3 |_ n / 2 _| comparisons.

  If n is even, we perform 1 initial comparison followed by 3(n - 2) /
  comparisons, for a total of 3n/2 -2. Thus, in either case, the total number of
  comparisons is at most 3|_n / 2_|.


- Selection in expected linear time

  The general selection problem appears more difficult than the simple problem
  of finding a minimum. Yet, surprisinly, the asymptotic running time for both
  problems is the same: 0(n). In this section, we present a divide-and-conquer
  algorithm for the selection problem.

  THe algorithm RANDOMIZED-SELECT is modeled after the quicksort algorithm of
  Chapter 7.

  As in quicksort, we partition the input array recursively. But, unlike
  quicksort, which recursively processes both sides of the partition,
  RANDOMIZED-SELECT works on only one side of the partition. This difference
  shows up in the analysis: whereas quicksort has an expected running time of
  0(n lg n), the expected running time of RANDOMIZED-SELECT is 0(n), assuming
  that the elements are distinct.


    RANDOMIZED-SELECT(A, p, r, i)
    1   if p == r
    2       return A[p]
    3   q = RANDOMIZED-PARTITION(A, p, r)
    4   k = q - p + 1

    5   if i == k // the pivot value is the answer
    6       return A[q]

    7   elseif i < k
    8       return RANDOMIZED-SELECT(A, p, q - 1, i)

    9   else return RANDOMIZED-SELECT(A, q + 1, r, i - k)

  The RANDOMIZED-SELECT procedure works as follows. Line 1 checks for the base
  case of recursion, in which the subarray A[p..r] consists of just one element.
  In this case, i must equal 1, and we simply return A[p] in line 2 as the i th
  smallest element.

  Otherwise, the call to RANDOMIZED-PARTITION in line 3 partitions A[q+1..r]
  such that each element of A[p..q - 1] is less than or equal to A[q], which in
  turn is less than each element of A[q+1..r].

  As in quicksort, we will refer to A[q] as the pivot element.

  Line 4 computes the number k of elements in the subarray A[p..q], that is, the
  number of elements in the subarray A[p..q], that is, the number of elements n
  the low side of the partition, plus one for the pivot element.

  Line 5 then checks whether A[q] is the i th smallest element. If it is, then
  line 6 returns A[q]. Otherwise, the algorithm determines in which of the two
  subarrays A[p..q - 1] and A[q+1..r] the i th smallest element lies.

  If i < k, then the desired element lies on the low side of the partition, and
  line 8 recursively selects it from the subarray.

  If i > k, however, then the desired element lies on the high side of the
  partition.

  Since we already know k values that are smaller than the i th smallest element
  of A[p..r] - namely, the elements of A[p..q] - the desired element is the (i
  - k) th smallest element of A[q+1..r], which line 9 finds recursively. The
  code appears to allow recursive calls to subarrays with 0 elements.

  The worst-case running time for RANDOMIZED-SELECT is 0(n^2), even to find the
  minimum, because we could be extremely unlucky and always partition around the
  largest remaining element, and partitioning takes 0(n) time. We will see that
  the algorithm has a linear expected running time, though, and becuase it is
  randomized, no particular input elicits the worst-case behaviour.

  [analysis on pg 217]

- Selection in worst-case linear time

  We now examine a selection algorithm whose running time is O9n) in the worst
  case. Like RANDOMIZED-SELECT, the algorithm SELECT finds the desired element
  by recursively partitioning the input array. Here, however, we guarantee a
  good split upon partitioning the array. SELECT uses the deterministic
  partitioning algorithm PARTITION from quicksort, but modified to take the
  element of partition around as an input parameter.

  [continued: pg 220]


* Data Structures

  Sets are as fundamental to computer science as they are to mathematics.
  Whereas mathematical sets are unchanging, the sets manipulated by algorithms
  can grow, shrink, or otherwise change over time. We call such sets dynamic.

  Algorithms may require several different types of operations to be performed
  on sets. For example, many algorithms need only the ability to insert elements
  into, delete elements from, and test membership in a set. We call a dynamic
  set that supports these operations a dictionary.

  The best way to implement a dynamic set depends upon the operations that must
  be supported.

- Elements of a dynamic set

  In a typical implementation of a dynamic set, each element is represented by
  an object whose attributes can be examined and manipulated if we have a
  pointer to the object.

  Some kinds of dynamic sets assume that one of the object’s attributes is an
  identifying key. If the keys are all different, we can think of the dynamic
  set as being a set of key values. The object may contain satellite data, which
  are carried around in other object attributes but are otherwise unused by the
  set implementation.

  It may also have attributes that are manipulated by the set operations; these
  attributes may contain data or pointers to other objects in the set.

  Some dynamic sets presuppose that the keys are drawn from a totally ordered
  set, such as the real numbers, or the set of all words under the usual
  alphabetic ordering. A total ordering allows us to define the minimum element
  of the set, for example, or to speak of the next element larger than a given
  element in a set.


- Operations on dynamic sets

  Operations on a dynamic set can be grouped into two categories: queries, which
  simply return information about the set, and modifying operations, which
  change the set.

    SEARCH(S, k)

    INSERT(S, x)

    DELETE(S, x)

    MINIMUM(S)

    MAXIMUM(S)

    SUCCESSOR(S, x)

    PREDECESSOR(S, x)

  We usually measure the time taken to execute a set operation in terms of the
  size of the set.


- Overview of Part iii

  Chapters 10–14 describe several data structures that we can use to implement
  dynamic sets; we shall use many of these later to construct efficient
  algorithms for a variety of problems. We already saw another important data
  structure—the heap—in Chapter 6.





* Elementary Data Structures

- Stacks and queues

  Stacks and queues are dynamic sets in which the element removed from the set
  by the D ELETE operation is prespecified. In a stack, the element deleted from
  the set is the one most recently inserted: the stack implements a last-in,
  first-out, or LIFO, policy. Similarly, in a queue, the element deleted is
  always the one that has been in the set for the longest time: the queue
  implements a first-in, first-out, or FIFO, policy. There are several efficient
  ways to implement stacks and queues on a computer. In this section we show how
  to use a simple array to implement each.

- Stacks

  The INSERT operation on a stack is often called P USH, and the DELETE opera-
  tion, which does not take an element argument, is often called POP.

  If we attempt to pop an empty stack, we say the stack underflows, which is
  normally an error.  If S:top exceeds n, the stack overflows.


- Queues

  We call the INSERT operation on a queue ENQUEUE, and we call the DELETE
  operation DEQUEUE; like the stack operation POP, DEQUEUE takes no element
  argument. The FIFO property of a queue causes it to operate like a line of
  customers waiting to pay a cashier. The queue has a head and a tail. When an
  element is enqueued, it takes its place at the tail of the queue, just as a
  newly arriving customer takes a place at the end of the line. The element
  dequeued is always the one at the head of the queue, like the customer at the
  head of the line who has waited the longest.

    ENQUEUE(Q, x)
    1   Q[Q.tail] = x
    2   if Q.tail == Q.length
    3       Q.tail = 1
    4   else Q.tail = Q.tail + 1

    DEQUEUE(Q)
    1   x = Q[Q.head]
    2   if Q.head == Q.length
    3       Q.head = 1
    4   else Q.head = Q.head + 1
    5   return x

    [pg 234 for picture]


- Linked Lists [pg 236]

  A linked list is a data structure in which the objects are arranged in a
  linear order. Unlike an array, however, in which the linear order is
  determined by the array indices, the order in a linked list is determined by a
  pointer in each object.  Linked lists provide a simple, flexible
  representation for dynamic sets, supporting (though not necessarily
  efficiently) all the operations listed on page 230.

  As shown in Figure 10.3, each element of a doubly linked list L is an object
  with an attribute key and two other pointer attributes: next and prev. The
  object may also contain other satellite data. Given an element x in the list,
  x.next points to its successor in the linked list, and x:pre points to its
  predecessor. If x.prev = NIL, the element x has no predecessor and is
  therefore the first element, or head, of the list. If x.next = NIL , the
  element x has no successor and is therefore the last element, or tail, of the
  list. An attribute L.head points to the first element of the list. If L.head =
  NIL , the list is empty.

  A list may have one of several forms. It may be either singly linked or doubly
  linked, it may be sorted or not, and it may be circular or not. If a list is
  singly linked, we omit the prev pointer in each element. If a list is sorted,
  the linear order of the list corresponds to the linear order of keys stored in
  elements of the list; the minimum element is then the head of the list, and
  the maximum element is the tail. If the list is unsorted, the elements can
  appear in any order. In a circular list, the prev pointer of the head of the
  list points to the tail, and the next pointer of the tail of the list points
  to the head.

  [pg 237 for diagram]


  We can think of a circular list as a ring of elements. In the remainder of
  this section, we assume that the lists with which we are working are unsorted
  and doubly linked.


- Searching for a linked list

    LIST-SEARCH(L, k)
    1   x = L.head
    2   while x != NIL and x.key != k
    3       x = x.next
    4   return x

  Takes 0(n) time in the worst case, since it may have to search an entire list.


- Inserting into a linked list

    LIST-INSERT(L, x)
    1   x.next = L.head // element L.head is now the second element
                        // x becomes the first element
    2   if L.head != NIL
    3       L.head.prev = x // make the second element prev point to x
    4   L.head = x // now update head so that it points to the first element
    5   x.prev = NIL

    [adds to the first item of the list -- beginning]
    [see pg 237 fig 10.3b]


  Running time for LIST-INSERT is O(1)

- Deleting from a linked list

    LIST-DELETE(L, x)
    1   if x.prev != NIL
    2       x.prev.next = x.next
    3   else L.head = x.next
    4   if x.next != NIL
    5       x.next.prev = x.prev

    [see pg 237 fig 10.3c]

- Sentinels

  The code for LIST-DELETE would be simpler if we could ignore the boundary
  conditions at the head and tail of the list:

    LIST-DELETE'(L, x)
    1   x.prev.next = x.next
    2   x.next.prev = x.prev

  A sentinel is a dummy object that allows us to simplify boundary conditions.
  For example, suppose that we provide with list L an object L.nil that
  represents NIL but has all the attributes of the other objects in the list.
  Wherever we have a reference to NIL in list code, we replace it by a reference
  to the sentinel L.nil.

  [pg 239 for diagram]

  This change turns a regular doubly linked list into a circular, doubly linked
  list with a sentinel, in which the sentinel L.nil lies between the head and
  tail. The attribute L.nil.next points to the head of the list, and L.nil.prev
  points to the tail. Similarly, both the next attribute of the tail and the pre
  attribute of the head point to L.nil. Since L.nil.next points to the head, we
  can eliminate the attribute L.head altogether, replacing references to it by
  references to L.nil.next. Figure 10.4(a) shows that an empty list consists of
  just the sentinel, and both L.nil.next and L.nil.prev point to L.nil.

    LIST-SEACH'(L, k)
    1   x = L.nil.next
    2   while x != L.nil and x.key != k
    3       x = x.next
    4   return x

    LIST-INSERT'(L, x)
    1   x.next = L.nil.next
    2   L.nil.next.prev = x
    3   L.nil.next = x
    4   x.prev = L.nil

  Sentinels rarely reduce the asymptotic time bounds of data structure
  operations, but they can reduce constant factors. The gain from using
  sentinels within loops is usually a matter of clarity of code rather than
  speed; the linked list code, for example, becomes simpler when we use
  sentinels, but we save only O(1) time in the LIST-INSERT' and LIST-DELETE'
  procedures. In other situations, however, the use of sentinels helps to
  tighten the code in a loop, thus reducing the coefficient of, say, n or n^2 in
  the running time.

  We should use sentinels judiciously. When there are many small lists, the
  extra storage used by their sentinels can represent significant wasted memory.
  In this book, we use sentinels only when they truly simplify the code.


- Implementing pointers and objects pg 242
